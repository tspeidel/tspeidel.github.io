[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Exploring Changes in Temperatures in Calgary\n\n\n\n\n\n\n\n\nJul 28, 2024\n\n\n\n\n\n\n\nAgreement Between Instruments\n\n\n\n\n\n\n\n\nAug 31, 2023\n\n\n\n\n\n\n\nA Changing Environment in Data Science\n\n\n\n\n\n\n\n\nAug 31, 2023\n\n\n\n\n\n\n\nPrediction vs. Inference Cultures: Same Toolbox, Different Focus\n\n\n\n\n\n\n\n\nAug 28, 2023\n\n\n\n\n\n\n\nAssessing the Effect of an Intervention\n\n\n\n\n\n\n\n\nAug 14, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-07-28-yyc-temperatures/index.html",
    "href": "posts/2024-07-28-yyc-temperatures/index.html",
    "title": "Exploring Changes in Temperatures in Calgary",
    "section": "",
    "text": "I am not a climate scientist… this analysis is conducted more for education than scientific inquiry. A heat spell in the summer of 2024 in Calgary, made me curious about how hot those days were compared to past years. As usual, I got carried away and took the opportunity to do more analyses, test modelling methods, illustrate the difference between prediction and explanation and, most importantly, have fun doing all of this."
  },
  {
    "objectID": "posts/2024-07-28-yyc-temperatures/index.html#a-first-model-with-splines",
    "href": "posts/2024-07-28-yyc-temperatures/index.html#a-first-model-with-splines",
    "title": "Exploring Changes in Temperatures in Calgary",
    "section": "A first model with splines",
    "text": "A first model with splines\nWe will attempt to model both and perhaps try to simplify the model later on. The code below generates the two sets of trigonometric functions for day and month, and then fits a quantile regression model of median temperatures using a restricted cubic spline to capture the long-term trend and the two sets of trigonometric functions to capture the two short-term trends. Finally, we add the predictions back to the data. Despite only using time, this is a somewhat complex model and given the cubic polynomial of the spline, the size of the data, and the quantile regression, it takes quite some time to converge. To address that, we will only evaluate the model on 20 years of data.\nThe use of quantile regression is because at some point we probably want to understand characteristics associated with extremes and quantile regression provides a powerful method to do so.\n\n\n\nCode\n# Prepare subset of data (20 years)\nyyc_20 &lt;- yyc_temp %&gt;% \n  filter(between(date, ymd(\"1953-01-01\"), ymd(\"1953-01-01\") + years(20)))\n\n\n# Extract time components from the time column\nyyc_20  &lt;- yyc_20 %&gt;%\n  mutate(time_num = as.numeric(time),\n         hour     = as.numeric(format(time, \"%H\")),\n         month    = as.numeric(format(time, \"%m\")))\n\n\n# Create sine and cosine transformations for hour and month\nyyc_20  &lt;- yyc_20  %&gt;%\n  mutate(sin_hour  = sin(2 * pi * (hour - 9) / 24),\n         cos_hour  = cos(2 * pi * (hour - 19) / 24),\n         sin_month = sin(2 * pi * (month - 4) / 12),\n         cos_month = cos(2 * pi * (month - 8) / 12))\n\n\n# Define the transformation function. Start with an arbitrary number of knots (5)\nk &lt;- attr(rcs(yyc_20 $time_num, 5), \"parms\")\nh &lt;- function(x) {\n  hour  &lt;- as.numeric(format(as.POSIXct(x, origin = \"1970-01-01\"), \"%H\"))\n  month &lt;- as.numeric(format(as.POSIXct(x, origin = \"1970-01-01\"), \"%m\"))\n  cbind(rcspline.eval(x, k), \n        sin_hour  = sin(2 * pi * (hour - 9) / 24), \n        cos_hour  = cos(2 * pi * (hour - 19) / 24),\n        sin_month = sin(2 * pi * (month - 4) / 12),\n        cos_month = cos(2 * pi * (month - 8) / 12))\n}\n\n\n# Fit the quantile regression model\ndd &lt;- datadist(yyc_20); options(datadist = 'dd')\nf1 &lt;- Rq(temp ~ gTrans(time_num, h), tau = 0.50, data = yyc_20)\n\n\n# Predictions\npred_f1 &lt;- yyc_20 %&gt;%\n  mutate(yhat_50 = predict(f1, newdata = yyc_20))\n\n\n\n\nCode\noptions(prType = \"html\")\nprint(f1, coefs = TRUE, table = FALSE, digits = 2)\n\n\n\nQuantile Regression      tau: 0.5\n\nRq(formula = temp ~ gTrans(time_num, h), tau = 0.5, data = yyc_20)\n\n\n\nFrequencies of Missing Values Due to Each Variable\n\n    temp time_num \n       2        0 \n\n\n\n\n\n\n\n\n\n\n\nDiscrimination\nIndex\n\n\n\n\nObs 175342.000\ng 10.6999\n\n\np 8.000\n\n\n\nResidual d.f. 175334.000\n\n\n\nmean |Y - Y^| 5.481\n\n\n\n\n\n\n\n\n\n\n\n\nβ\nS.E.\nt\nPr(&gt;|t|)\n\n\n\n\nIntercept\n  3.67\n 0.03\n108.51\n&lt;0.0001\n\n\ntime_num\n  0.00\n 0.00\n22.60\n&lt;0.0001\n\n\ntime_num'\n  0.00\n 0.00\n-23.85\n&lt;0.0001\n\n\ntime_num''\n  0.00\n 0.00\n23.46\n&lt;0.0001\n\n\nsin_hour\n  1.55\n 0.03\n54.64\n&lt;0.0001\n\n\ncos_hour\n -4.90\n 0.03\n-171.67\n&lt;0.0001\n\n\nsin_month\n 10.93\n 0.05\n205.90\n&lt;0.0001\n\n\ncos_month\n  1.62\n 0.05\n31.03\n&lt;0.0001\n\n\n\n\n\nCode\n# AIC(model)\n\n\n\nThe performance measure \\(\\text{mean } |Y - \\hat{Y}|\\) is the mean absolute deviation (MAD) and represents the average absolute prediction error. It is conceptually similar to RMSE in other linear models. A value of 5.48 tells us that average distance between the regression line and the point is ±5 degrees C°.\n\\(g\\) stands for \\(g-index\\) and represents the Gini mean difference, a measure of dispersion. A value of 10.7 suggests that the typical difference in median ambient temperatures between any two predictions is 11 degrees C°. This is useful to benchmark among multiple models. One could also compare the \\(g-index\\) from the model above to the Gini mean difference on the raw data which is 14. While this comparison suggests that the model underestimates the amount of variation in temperatures, it is more useful to use it as one of several measures of model comparisons.\n\n\nThe number of knots here (5) is rather arbitrary. We can tweak those later. Let’s visualize the results.\n\n\nCode\npred_f1 %&gt;% \n  ggplot(aes(x = time, y = temp)) + \n  geom_point(alpha = 0.05, shape = 1, size = 0.5) +\n  geom_line(aes(x = time, y = yhat_50), linewidth = 0.1, alpha = 1, color = \"#FF0066\") +\n  scale_fill_viridis_d() +\n  scale_x_datetime(date_labels = \"%Y\", breaks = seq(ymd_hms(\"1950-01-01 00:00:00\"), ymd_hms(\"2025-01-01 00:00:00\"), by = \"5 years\")) +\n  scale_y_continuous(breaks = seq(-40, 40, 20), limits = c(-40, 40)) +\n  labs(x = \"\",\n       y                = \"Mean Hourly Temperature (C°)\",\n       title            = \"\",\n       caption          = \"Data: Environment and Climate Change Canada (2024). Historical Climate Data. Government of Canada.\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nFigure 14: Plot of temperatures over time with superimposed predictions of median temperatures from the model.\n\n\n\n\n\n\nThis looks promising, but let us take a closer look at a time window to assess the adequacy of the seasonalities:\n\n\nCode\npred_f1 %&gt;% \n  filter(between(time, ymd_hms(\"1965-07-01 00:00:00\"), ymd_hms(\"1965-07-06 23:00:00\"))) %&gt;% \n  ggplot(aes(x = time, y = temp)) + \n  geom_point(alpha = 1, shape = 1, size = 2.6) +\n  geom_line(aes(x = time, y = yhat_50), linewidth = 0.5, alpha = 1, color = \"#FF0066\") +\n  scale_fill_viridis_d() +\n  scale_x_datetime(breaks = \"12 hours\", date_labels = \"%b-%-d-%Y\\nHour: %H\") +\n  scale_y_continuous(breaks = seq(0, 30, 5), limits = c(0, 30)) +\n  labs(x = \"\",\n       y                = \"Mean Hourly Temperature (C°)\",\n       title            = \"\",\n       caption          = \"Data: Environment and Climate Change Canada (2024). Historical Climate Data. Government of Canada.\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nFigure 15: Plot of temperatures over time with superimposed predictions of median temperatures from the model. Focusing in on a smaller time window to assess seasonalities.\n\n\n\n\n\n\nThe daily seasonality appears adequate. Let us now look at the annual seasonality:\n\n\nCode\npred_f1 %&gt;% \n  filter(between(time, ymd_hms(\"1965-01-01 00:00:00\"), ymd_hms(\"1966-12-31 23:00:00\"))) %&gt;% \n  ggplot(aes(x = time, y = temp)) + \n  geom_point(alpha = 0.1, shape = 1, size = 1.2) +\n  geom_line(aes(x = time, y = yhat_50), linewidth = 0.1, alpha = 1, color = \"#FF0066\") +\n  scale_fill_viridis_d() +\n  scale_x_datetime(breaks = \"1 month\", date_labels = \"%b\\n%Y\") +\n  scale_y_continuous() +\n  labs(x = \"\",\n       y                = \"Mean Hourly Temperature (C°)\",\n       title            = \"Mean Hourly Temperature 1954-2024\",\n       caption          = \"Data: Environment and Climate Change Canada (2024). Historical Climate Data. Government of Canada.\") +\n  coord_cartesian(expand = FALSE) +\n  theme(legend.position = \"none\") +\n  theme(plot.subtitle = element_markdown())\n\n\n\n\n\n\n\n\nFigure 16: Plot of temperatures over time with superimposed predictions of median temperatures from the model. Focusing in on a smaller time window to assess seasonalities.\n\n\n\n\n\nNot great… The annual seasonality appear irregular because we used categorical months, causing abrupt transitions from one month to the next. Although this method captures the annual seasonality, it doesn’t reflect the smooth changes of a natural system. To address this, we evaluate using days instead in the next section."
  },
  {
    "objectID": "posts/2024-07-28-yyc-temperatures/index.html#a-second-model-with-splines",
    "href": "posts/2024-07-28-yyc-temperatures/index.html#a-second-model-with-splines",
    "title": "Exploring Changes in Temperatures in Calgary",
    "section": "A second model with splines",
    "text": "A second model with splines\nWe now try to smooth the monthly seasonality.\n\n\nCode\n# Ensure data has the necessary columns\nyyc_20 &lt;- yyc_20 %&gt;%\n  select(-starts_with(\"sin\"), -starts_with(\"cos\")) %&gt;% \n  mutate(time_num = as.numeric(time),\n         hour     = as.numeric(format(time, \"%H\")),\n         doy      = as.numeric(format(time, \"%j\"))\n         )\n\n# Create sine and cosine transformations for hour and day of the year\nyyc_20 &lt;- yyc_20 %&gt;%\n  mutate(sin_hour = sin(2 * pi * (hour - 9) / 24),\n         cos_hour = cos(2 * pi * (hour - 19) / 24),\n         sin_doy  = sin(2 * pi * (doy - 113) / 365),\n         cos_doy  = cos(2 * pi * (doy - 174) / 365))\n\n# Define the transformation function\nk &lt;- attr(rcs(yyc_20$time_num, 5), \"parms\")\nh &lt;- function(x) {\n  hour &lt;- as.numeric(format(as.POSIXct(x, origin = \"1970-01-01\"), \"%H\"))\n  doy &lt;- as.numeric(format(as.POSIXct(x, origin = \"1970-01-01\"), \"%j\"))\n  cbind(rcspline.eval(x, k), \n        sin_hour = sin(2 * pi * (hour - 9) / 24), \n        cos_hour = cos(2 * pi * (hour - 19) / 24),\n        sin_doy  = sin(2 * pi * (doy - 113) / 365),\n        cos_doy  = cos(2 * pi * (doy - 174) / 365))\n}\n\n# Fit the quantile regression model\ndd &lt;- datadist(yyc_20); options(datadist = 'dd')\nf2 &lt;- Rq(temp ~ gTrans(time_num, h), tau = 0.50, data = yyc_20)\n\n# Predictions\npred_f2 &lt;- yyc_20 %&gt;%\n  mutate(yhat_50 = predict(f2, newdata = yyc_20))\n\n\n\n\nCode\npred_f2 %&gt;% \n  filter(between(time, ymd_hms(\"1965-01-01 00:00:00\"), ymd_hms(\"1966-12-31 23:00:00\"))) %&gt;% \n  ggplot(aes(x = time, y = temp)) + \n  geom_point(alpha = 0.1, shape = 1, size = 1.2) +\n  geom_line(aes(x = time, y = yhat_50), linewidth = 0.1, alpha = 1, color = \"#FF0066\") +\n  scale_fill_viridis_d() +\n  scale_x_datetime(breaks = \"1 month\", date_labels = \"%b\\n%Y\") +\n  scale_y_continuous() +\n  labs(x = \"\",\n       y                = \"Mean Hourly Temperature (C°)\",\n       title            = \"\",\n       caption          = \"Data: Environment and Climate Change Canada (2024). Historical Climate Data. Government of Canada.\") +\n  coord_cartesian(expand = FALSE) +\n  theme(legend.position = \"none\") +\n  theme(plot.subtitle = element_markdown())\n\n\n\n\n\n\n\n\nFigure 17: Plot of temperatures over time with superimposed predictions of median temperatures from the model. Here the focus is on a time window (2020 to 2020) to assess the adequacy of the annual seasonality.\n\n\n\n\n\n\nMuch better. We now have what seems like a reasonable model at face value. Now, let’s tweak the model by revisiting the number of knots. When using splines, unless we have a strong prior knowledge to do otherwise, knots are ideally placed at quantile distribution of the data (see Harrell2, Stone3).\n2 Harrell, F. E. (2013). Regression modelling strategies: With applications to linear models, logistic regression, and survival analysis. Springer Science & Business Media.3 Stone, C. J. (1986). [Generalized additive models]: Comment. Statistical Science, 1(3). https://doi.org/10.1214/ss/1177013607\nAn example of custom knot placement is when one knows of a physical change occurring at a precise location. For instance, in physics you might place a knot at the location of a physical state transition like when going from liquid to solid. Another example in a quasi-experiment is to place the knot at the time of an intervention.\n4 Logistic regression diagnostics when predictors all have skewed distributions. (n.d.). Cross Validated. https://stats.stackexchange.com/questions/67078/logistic-regression-diagnostics-when-predictors-all-have-skewed-distributions\nBut what about the number? In most of the literature, it is common to see 3 to 7 knots. However, in our case we have a lot of data and with time variables, is not uncommon to utilize more knots. One rule of thumb I seem to recall is to have ~5 knots per “era”. It’s hard to think of “era” in climate data with a mere 70 years of data. A data-driven strategy is to let AIC guide the number of knots4.\nIn the next section, I will evaluate the optimal number of knots. For convenience, instead of using the computationally intensive quantile regression, I will use an ordinary least square. For knots increments, since we are dealing with sexagesimal data, let us start with 12 and increment by 6 or 12.\n\n\n\nCode\n## Not sure parallelizing helps:\nlibrary(parallel)\nnum_cores &lt;- detectCores() - 1\n\n## Create an empty data frame to store the results\nresults_1 &lt;- data.frame(i              = numeric(),\n                        AIC            = numeric(),\n                        adj_r_squared  = numeric(),\n                        g              = numeric())\n\n## Loop through sequence of knots and fit the model. Continue on if no convergence for some of the k's.\n## i = sequence of knots\n## k = vector of knot locations, each i\n## Use ols for speed instead of the computationally intensive Rq\n\nfor (i in seq(12, 120, 12)) {\n  k    &lt;- attr(rcs(yyc_temp$time_num, i), \"parms\")\n  h    &lt;- function(x) {\n    hour &lt;- as.numeric(format(as.POSIXct(x, origin = \"1970-01-01\"), \"%H\"))\n    doy  &lt;- as.numeric(format(as.POSIXct(x, origin = \"1970-01-01\"), \"%j\"))\n    cbind(rcspline.eval(x, k), \n          sin_hour = sin(2 * pi * (hour - 9) / 24), \n          cos_hour = cos(2 * pi * (hour - 19) / 24),\n          sin_doy  = sin(2 * pi * doy / 365),\n          cos_doy  = cos(2 * pi * doy / 365))\n  }\n  tryCatch({\n    f             &lt;- ols(temp ~ gTrans(time_num, h), data = yyc_temp)\n    aic_value     &lt;- AIC(f)\n    adj_r_squared &lt;- f$stats['adjR2'] \n    g             &lt;- f$stats['g']    \n    # Append the results to the data frame\n    results_1 &lt;- rbind(results_1, data.frame(i = i, AIC = aic_value, adj_r_squared = adj_r_squared, g = g))\n  }, error = function(e) {\n    # Print a message and continue to the next iteration if an error occurs\n    message(paste(\"Model did not converge for i = \", i))\n  })\n}\n\n\n\n\nCode\nresults_1 %&gt;% \n  bind_rows(results_2, results_3) %&gt;% \n  ggplot(aes(y = AIC, x = i)) +\n  geom_point(shape = 1, size = 1.6) +\n  geom_line() +\n  labs(x     = \"Number of knots\",\n       y     = \"AIC (lower is better)\",\n       title = \"Optimal Number of Knots for Long-Term Trend\") \n\n\n\n\nFor completeness, I also evaluated the fit from a quantile GAM. The results are not shown because it was nearly identical to the spline approach. After all, the two approaches are conceptually very similar, both relying on smoothing splines.\nThe above approach of using AIC to determine the ideal number of knots seems to be a dead end. The dataset is huge and so is the number of knots needed to minimize AIC and my machine runs out of memory at approximately 300 knots. Besides, I would not feel comfortable assigning so many degrees of freedom to a long term trend of a mere 70 years. After all, I am trying to capture the long term trend and we don’t need wiggleness. I decide to stop here."
  },
  {
    "objectID": "posts/2024-07-28-yyc-temperatures/index.html#residuals-analysis",
    "href": "posts/2024-07-28-yyc-temperatures/index.html#residuals-analysis",
    "title": "Exploring Changes in Temperatures in Calgary",
    "section": "Residuals Analysis",
    "text": "Residuals Analysis\nNext, let’s take a look at the residuals. While quantile regression does not impose stringent assumptions like some linear regression, examination of residuals can still provide valuable insight on understanding and improving the model.\n\n\nCode\n# Predictions + residuals\nresiduals &lt;- resid(f2) %&gt;%\n  as.data.frame() %&gt;%\n  select(residuals = V1) %&gt;% \n  ungroup()\n\npred_f2 &lt;- pred_f2 %&gt;%\n  bind_cols(residuals)\n\n\n\n\nCode\n## Plot of residuals over time\ngg1 &lt;- pred_f2 %&gt;% \n  filter(between(time, ymd_hms(\"1965-07-01 00:00:00\"), ymd_hms(\"1965:09:01 00:00:00\"))) %&gt;% \n  ggplot(aes(x = time, y = residuals)) + \n  geom_point(alpha = 0.1, shape = 1, size = 1.2) +\n  geom_smooth(aes(x = time, y = residuals), se = FALSE, linewidth = 0.7, alpha = 1, color = \"#00b7ec\") +\n  scale_x_datetime(breaks = \"1 week\", date_labels = \"%b %d\\n%Y\") +\n  scale_y_continuous() +\n  labs(x = \"\",\n       y                = \"Residuals of Predicted Median Temperature (C°)\",\n       title            = \"Residuals of Predicted Median Temperature 1954-2024\") +\n  coord_cartesian(expand = FALSE) +\n  theme(legend.position = \"none\") +\n  theme(plot.subtitle = element_markdown())\n\n## Autocorrelation of residuals\ngg2 &lt;- pred_f2 %&gt;%\n  tsibble::as_tsibble(index = time) %&gt;% \n  tsibble::fill_gaps(.full = TRUE) %&gt;% \n  feasts::ACF(residuals, lag_max = 1440) %&gt;%\n  feasts::autoplot() +\n  ggtitle(\"ACF of Residuals\") +\n  scale_x_continuous(breaks = seq(0, 1440, by = 24*7)) \n\ngg3 &lt;- pred_f2 %&gt;%\n  tsibble::as_tsibble(index = time) %&gt;% \n  tsibble::fill_gaps(.full = TRUE) %&gt;% \n  feasts::PACF(residuals, lag_max = 1440) %&gt;%\n  feasts::autoplot() +\n  ggtitle(\"PACF of Residuals\") +\n  scale_x_continuous(breaks = seq(0, 1440, by = 24*7)) \n\n## Assemble graphs\ngg1 / (gg2 | gg3)\n\n\n## If running into problems\n# gc()\n# gc(reset = TRUE)\n# dev.off()\n\n\n\n\n\n\n\n\nFigure 18: Plot of residuals diagnostic. The residual plot over time (top graph) show some problems with seasonality that are unexplained by the model, although with a mean of 0. The ACF and PACF are more problematic as they signal lack of i.i.d.\n\n\n\n\n\nThe diagnostic is not great. While the residual plot roughly wraps around the zero mean, there clearly are unexplained seasonalities. The ACF reveals significant autocorrelation lasting approximately 2 weeks (336 hours). The PACF reveals similar problems."
  },
  {
    "objectID": "posts/2024-07-28-yyc-temperatures/index.html#a-third-model-with-splines-and-lag-terms",
    "href": "posts/2024-07-28-yyc-temperatures/index.html#a-third-model-with-splines-and-lag-terms",
    "title": "Exploring Changes in Temperatures in Calgary",
    "section": "A third model with splines and lag terms",
    "text": "A third model with splines and lag terms\nLet’s now explore adding lag terms. It always feels like cheating to add lags of the response, though this is not too different than fitting an autoregressive model. This is a good opportunity to highlight the different objectives in modelling: prediction versus inference.\nSo far, I have aimed to fit a model that captures both short-term seasonal patterns and long-term trends, focusing on improving predictive accuracy. I have no interest in making predictions about the future. Simultaneously, I have been mindful of aspects of the models that ensure valid inference, such as addressing heteroskedasticity and ensuring the independence and identical distribution (i.i.d.) of the response and residuals (even though some of those assumptions are relaxed in quantile regression). Incidentally, addressing those things also helped improving predictive accuracy.\nWhen I refer to inference, I mean it in its traditional sense: the process of drawing conclusions about a population from a sample. This contrasts with the more prediction-focused bend of the term often seen in machine learning community. Ultimately, my goal is to develop a reasonably good model that can serve as a foundation for making inferences about changes in temperatures over the past 70 years.\nI also do something else, not shown below. Separately, I evaluated all lags in increments of 3 from 3 to 48 and then used the LASSO to help identify which lags were more important. These were (in order) lag 3, 24, 27, 6 and 45. While I do not like using LASSO for variable selection, I think it’s ok to use in this context.\nLastly, I take the opportunity to reign-in the number of knots for the long-term trend.\n\n\nCode\n# Ensure tmp has the necessary columns\nyyc_20 &lt;- yyc_20 %&gt;%\n  select(-starts_with(\"sin\"), -starts_with(\"cos\")) %&gt;% \n  mutate(time_num = as.numeric(time),\n         hour     = as.numeric(format(time, \"%H\")),\n         doy      = as.numeric(format(time, \"%j\"))\n  )\n\n# Create sine and cosine transformations for hour and day of the year\nyyc_20 &lt;- yyc_20 %&gt;%\n  mutate(sin_hour = sin(2 * pi * (hour - 9)  / 24),\n         cos_hour = cos(2 * pi * (hour - 19) / 24),\n         sin_doy  = sin(2 * pi * (doy - 113) / 365),\n         cos_doy  = cos(2 * pi * (doy - 174) / 365))\n\n# Define the transformation function\nk &lt;- attr(rcs(yyc_20$time_num, 5), \"parms\")\nh &lt;- function(x) {\n  hour &lt;- as.numeric(format(as.POSIXct(x, origin = \"1970-01-01\"), \"%H\"))\n  doy &lt;- as.numeric(format(as.POSIXct(x, origin = \"1970-01-01\"), \"%j\"))\n  cbind(rcspline.eval(x, k), \n        sin_hour = sin(2 * pi * (hour - 9)  / 24), \n        cos_hour = cos(2 * pi * (hour - 19) / 24),\n        sin_doy  = sin(2 * pi * (doy - 113) / 365),\n        cos_doy  = cos(2 * pi * (doy - 174) / 365))\n}\n\nyyc_20 &lt;- yyc_20 %&gt;% \n  mutate(lag_3  = lag(temp, 3),\n         lag_6  = lag(temp, 6),\n         lag_24 = lag(temp, 24),\n         lag_27 = lag(temp, 27),\n         lag_45 = lag(temp, 45),\n  )\n\n## Add a lag\ndd &lt;- datadist(yyc_20); options(datadist = 'dd')\nf3_50 &lt;- Rq(temp ~ gTrans(time_num, h) + lag_3 + lag_6 + lag_24 + lag_27 + lag_45,\n                method = \"br\",\n                tau    = 0.50,\n                data   = yyc_20)\n\nf3_90 &lt;- Rq(temp ~ gTrans(time_num, h) + lag_3 + lag_6 + lag_24 + lag_27 + lag_45,\n                method = \"br\",\n                tau    = 0.90,\n                data   = yyc_20)\n\nf3_10 &lt;- Rq(temp ~ gTrans(time_num, h) + lag_3 + lag_6 + lag_24 + lag_27 + lag_45,\n                method = \"br\",\n                tau    = 0.10,\n                data   = yyc_20)\n\n\n# Predictions\npred_f3 &lt;- yyc_20 %&gt;%\n  mutate(yhat_10 = predict(f3_10, newdata = yyc_20)) %&gt;% \n  mutate(yhat_50 = predict(f3_50, newdata = yyc_20)) %&gt;% \n  mutate(yhat_90 = predict(f3_90, newdata = yyc_20))\n\n\n# preds10 &lt;- predict(model3_10, newdata = yyc_temp, se.fit = TRUE)\n# preds50 &lt;- predict(model3_50, newdata = yyc_temp, se.fit = TRUE)\n# preds90 &lt;- predict(model3_90, newdata = yyc_temp, se.fit = TRUE)\n# \n# yyc_temp &lt;- yyc_temp %&gt;%\n#   bind_cols(preds50) %&gt;% \n#   rename(yhat50 = linear.predictors,\n#          yhat50_se = se.fit) %&gt;% \n#   bind_cols(preds10) %&gt;% \n#   rename(yhat10 = linear.predictors,\n#          yhat10_se = se.fit) %&gt;%   \n#   bind_cols(preds90) %&gt;% \n#   rename(yhat90 = linear.predictors,\n#          yhat90_se = se.fit)\n\n## Calculate the 90% confidence interval for each quantile\n# alpha &lt;- 0.10\n# z_value &lt;- qnorm(1 - alpha / 2)\n\n# Calculate lower and upper bounds of the confidence interval\n# yyc_temp &lt;- yyc_temp %&gt;%\n#   mutate(yhat50_lo = yhat50 - z_value * yhat50_se) %&gt;% \n#   mutate(yhat50_hi = yhat50 + z_value * yhat50_se) %&gt;% \n#   mutate(yhat10_lo = yhat10 - z_value * yhat10_se) %&gt;% \n#   mutate(yhat10_hi = yhat10 + z_value * yhat10_se) %&gt;% \n#   mutate(yhat90_lo = yhat90 - z_value * yhat90_se) %&gt;% \n#   mutate(yhat90_hi = yhat90 + z_value * yhat90_se)\n\n\n\n## Run in interactive session and cache model object for efficiency\n# save(model3_50, model3_10, model3_90, file = \"posts/2024-07-28-yyc-temperatures/Data/model3.RData\", compress = \"xz\", compression_level = 9)\n# save(yyc_temp, file = \"posts/2024-07-28-yyc-temperatures/Data/yyc_temp.RData\", compress = \"xz\", compression_level = 9)\n\n## Print model\noptions(prType = \"html\")\nprint(f3_50, coefs = FALSE, table = FALSE, digits = 2)\n\n\n\nQuantile Regression      tau: 0.5\n\nRq(formula = temp ~ gTrans(time_num, h) + lag_3 + lag_6 + lag_24 + \n    lag_27 + lag_45, tau = 0.5, data = yyc_20, method = \"br\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscrimination\nIndex\n\n\n\n\nObs 175287.000\ng 13.61221\n\n\np 13.000\n\n\n\nResidual d.f. 175274.000\n\n\n\nmean |Y - Y^| 1.861\n\n\n\n\n\n\n\n\n\nThe model improves noticeably. Mean absolute deviation went from 5.4 to 1.9. Let’s take another look at the residuals.\n\n\nCode\n## Generate resid\ngg4 &lt;- pred_f3  %&gt;%\n  mutate(residuals = resid(f3_50)) %&gt;%\n  as_tsibble(index = time) %&gt;% \n  fill_gaps(.full = TRUE) %&gt;% \n  ACF(residuals, lag_max = 168) %&gt;%\n  autoplot() +\n  ggtitle(\"ACF of Residuals\") +\n  scale_x_continuous(breaks = seq(0, 168, by = 24))\n\ngg5 &lt;- pred_f3  %&gt;%\n  mutate(residuals = resid(f3_50)) %&gt;%\n  as_tsibble(index = time) %&gt;% \n  fill_gaps(.full = TRUE) %&gt;% \n  PACF(residuals, lag_max = 168) %&gt;%\n  autoplot() +\n  ggtitle(\"PACF of Residuals\") +\n  scale_x_continuous(breaks = seq(0, 168, by = 24))\n\n## Assemble graphs\ngg4 / gg5\n\n\n\n\n\n\n\n\nFigure 19: Plot of residuals diagnostic. The ACF and PACF have improved considerably with the inclusion of lags, though they still display some autocorrelation issues.\n\n\n\n\n\n\nAlso much batter, though still short of ideal. Finally, let’s take a look at the predictions:\n\n\nCode\npred_f3 %&gt;% \n  filter(between(time, ymd_hms(\"1960-01-01 00:00:00\"), ymd_hms(\"1961-12-31 23:00:00\"))) %&gt;% \n  ggplot(aes(x = time, y = temp)) + \n  geom_point(alpha = 0.1, shape = 1, size = 1.2) +\n  geom_line(aes(x = time, y = yhat_50), linewidth = 0.1, alpha = 1, color = \"#FF0066\") +\n  scale_fill_viridis_d() +\n  scale_x_datetime(breaks = \"1 month\", date_labels = \"%b\\n%Y\") +\n  scale_y_continuous() +\n  labs(x = \"\",\n       y                = \"Mean Hourly Temperature (C°)\",\n       title            = \"Mean Hourly Temperature 1954-2024\",\n       caption          = \"Data: Environment and Climate Change Canada (2024). Historical Climate Data. Government of Canada.\") +\n  coord_cartesian(expand = FALSE) +\n  theme(legend.position = \"none\") +\n  theme(plot.subtitle = element_markdown())\n\n\n\n\n\n\n\n\nFigure 20: Plot of predictions showing improved fit.\n\n\n\n\n\n\n\nCode\npred_f3 %&gt;% \n  filter(between(time, ymd_hms(\"1965-07-01 00:00:00\"), ymd_hms(\"1965-07-07 23:00:00\"))) %&gt;% \n  ggplot(aes(x = time, y = temp)) + \n  geom_point(alpha = 0.75, shape = 1, size = 3.5) +\n  geom_line(aes(x = time, y = yhat_50), linewidth = 0.9, alpha = 0.6, color = \"#FF0066\") +\n  geom_line(aes(x = time, y = yhat_90), linewidth = 0.5, alpha = 0.4, color = \"#00cc7a\") +\n  geom_line(aes(x = time, y = yhat_10), linewidth = 0.5, alpha = 0.4, color = \"#7a00cc\") +\n  scale_fill_viridis_d() +\n  scale_x_datetime(breaks = \"2 days\", date_labels = \"%b, %d\\n%Y\") +\n  scale_y_continuous() +\n  labs(x = \"\",\n       y                = \"Mean Hourly Temperature (C°)\",\n       title            = \"Mean Hourly Temperature 1954-2024\",\n       caption          = \"Data: Environment and Climate Change Canada (2024). Historical Climate Data. Government of Canada.\") +\n  coord_cartesian(expand = FALSE) +\n  theme(legend.position = \"none\") +\n  theme(plot.subtitle = element_markdown())\n\n\n\n\n\n\n\n\nFigure 21: Plot of predictions showing improved and additional 10th and 90th percentiles.\n\n\n\n\n\n\nPrediction look visibly improved. However, this may be a good model for prediction, but not a great model for inference because it’s too data driven. If the model explains almost all of the variability in the response, it may not leave room for estimating uncertainty. Inferential statistics often relies on variability to make inferences about population parameters, and a perfect model can obscure this variability."
  },
  {
    "objectID": "posts/2024-07-28-yyc-temperatures/index.html#summary-of-predictive-models",
    "href": "posts/2024-07-28-yyc-temperatures/index.html#summary-of-predictive-models",
    "title": "Exploring Changes in Temperatures in Calgary",
    "section": "Summary of Predictive Models",
    "text": "Summary of Predictive Models\nIn the sections above, I attempted to fit a reasonable predictive model that may also serve for inferential purposes. Ultimately though, what makes a powerful predictive model is the inclusion of lags which will hurt inference and interpretation. Surely, one could continue improving these models further.\nI would refer to the work of Laurinec or Simpson, both of whom illustrate the use of GAMs in handling seasonal patterns and complex relationships within data. Their approaches are in the same philosophy of this post, since they provide a good trade-off between predictive strength, sound statistical properties and interpretability (besides being a lot less computationally intensive than data hungry models like neural networks!)."
  },
  {
    "objectID": "posts/2024-07-28-yyc-temperatures/index.html#simulating-the-effect-of-lags-on-inference",
    "href": "posts/2024-07-28-yyc-temperatures/index.html#simulating-the-effect-of-lags-on-inference",
    "title": "Exploring Changes in Temperatures in Calgary",
    "section": "Simulating the Effect of Lags on Inference",
    "text": "Simulating the Effect of Lags on Inference\n\n\nCode\n# Take a subset of the data to avoid significant p-values for trivially small effect size caused by huge dataset\nset.seed(321)\n\nsim &lt;- yyc_temp %&gt;%\n  filter(between(date, ymd(\"1970-01-01\"), ymd(\"1970-12-31\"))) %&gt;% \n  drop_na() %&gt;% \n  mutate(simulated = 0.05 * temp + rnorm(n(), sd = 5))\n\n# Check correlation\n# round(cor(sim$temp, sim$simulated, method = \"spearman\", use = \"complete.obs\"), 2)\n\n\nTo do a demonstration of the damaging effect of lags on inference, let us simulate a variable, call it simulated, that is weakly correlated to temperatures. We aim for a Spearman correlation of about 0.10 (the actual from simulated data is 0.11). We also take a 1-year subset of the data to avoid trivially small effects being significant, a side effect of a huge dataset (new dataset size n=8760. Notice that this is still a very big dataset).\n\n\nCode\n# Generate necessary columns\nsim &lt;- sim %&gt;%\n  mutate(time_num = as.numeric(time),\n         hour     = as.numeric(format(time, \"%H\")),\n         doy      = as.numeric(format(time, \"%j\"))\n  )\n\n# Generate lags as before\nsim &lt;- sim %&gt;% \n  mutate(temp_lag_3  = lag(temp, 3),\n         temp_lag_6  = lag(temp, 6),\n         temp_lag_24 = lag(temp, 24),\n         temp_lag_27 = lag(temp, 27),\n         temp_lag_45 = lag(temp, 45),\n  )\n\n## Remove NAs\nsim &lt;- sim %&gt;% \n  drop_na()\n  \n## Fit the models\ndd &lt;- datadist(sim); options(datadist = 'dd')\nf_sim &lt;- ols(temp ~ simulated + time_num + temp_lag_3 + temp_lag_6 + temp_lag_24 + temp_lag_27 + temp_lag_45, data = sim)\nf_no_lags &lt;- ols(temp ~ simulated + time_num, data = sim)\n\n\n\n\nCode\n## Print model with lags\noptions(prType = \"html\")\n# anova(f_sim, digits = 2)\nprint(f_sim, coefs = TRUE, title = \"Model with lags\")\n## Print model no lags\noptions(prType = \"html\")\n# anova(f_no_lags,  digits = 2)\nprint(f_no_lags, coefs = TRUE, title = \"Model without lags\")\n\n\n\n\n\n\n\n\n\n\n\nModel with lags\n\nols(formula = temp ~ simulated + time_num + temp_lag_3 + temp_lag_6 + \n    temp_lag_24 + temp_lag_27 + temp_lag_45, data = sim)\n\n\n\n\n\n\nModel LikelihoodRatio Test\nDiscriminationIndexes\n\n\n\n\nObs 8715\nLR χ2 27444.47\nR2 0.957\n\n\nσ 2.6778\nd.f. 7\nR2adj 0.957\n\n\nd.f. 8707\nPr(&gt;χ2) 0.0000\ng 14.269\n\n\n\n\n\nResiduals\n\n      Min        1Q    Median        3Q       Max \n-15.29342  -1.62018  -0.08236   1.54457  13.12690 \n\n\n\n\n\n\nβ\nS.E.\nt\nPr(&gt;|t|)\n\n\n\n\nIntercept\n  0.0525\n 0.0579\n  0.91\n0.3645\n\n\nsimulated\n  0.0094\n 0.0058\n  1.63\n0.1022\n\n\ntime_num\n  0.0000\n 0.0000\n -0.22\n0.8254\n\n\ntemp_lag_3\n  1.0133\n 0.0095\n106.19\n&lt;0.0001\n\n\ntemp_lag_6\n -0.1716\n 0.0088\n-19.53\n&lt;0.0001\n\n\ntemp_lag_24\n  0.4429\n 0.0100\n 44.24\n&lt;0.0001\n\n\ntemp_lag_27\n -0.4142\n 0.0096\n-43.36\n&lt;0.0001\n\n\ntemp_lag_45\n  0.1163\n 0.0052\n 22.42\n&lt;0.0001\n\n\n\n\n\n\n(a) Table of coefficient and performance for the model using lags. The model uses a simulated variable weakly associated with temperatures. Notice the somewhat large standard error and the non-significant p-value for the simulated variable. The fit of the model is high with an R2 of 0.96.\n\n\n\n\n\n\n\n\n\n\nModel without lags\n\nols(formula = temp ~ simulated + time_num, data = sim)\n\n\n\n\n\n\nModel LikelihoodRatio Test\nDiscriminationIndexes\n\n\n\n\nObs 8715\nLR χ2 183.78\nR2 0.021\n\n\nσ 12.7906\nd.f. 2\nR2adj 0.021\n\n\nd.f. 8712\nPr(&gt;χ2) 0.0000\ng 2.116\n\n\n\n\n\nResiduals\n\n     Min       1Q   Median       3Q      Max \n-40.5069  -7.2276   0.6132   9.3244  31.0185 \n\n\n\n\n\n\nβ\nS.E.\nt\nPr(&gt;|t|)\n\n\n\n\nIntercept\n 1.3005\n 0.2762\n 4.71\n&lt;0.0001\n\n\nsimulated\n 0.2881\n 0.0273\n10.55\n&lt;0.0001\n\n\ntime_num\n 0.0000\n 0.0000\n 8.61\n&lt;0.0001\n\n\n\n\n\n\n(b) Table of coefficient and performance for the model without lags. The model uses a simulated variable weakly associated with temperatures. Compared to Table 8 (a), the standard error became 10 times smaller compared to its coefficient and the p-value is highly significant. The fit of the model is low with an R2 of 0.02, as it would be expected from a weakly correlated predictor.\n\n\n\n\n\n\n\nTable 4\n\n\n\n\n\nLet’s examine the three key points raised in the Achen paper in the context of these models:\n\nLagged response variables have no obvious causal interpretation. This is self explanatory: while lagged variables are valuable for forecasting and capturing temporal dependencies, they rarely imply causality. Using the body temperature analogy: while a person’s temperature at a given time is correlated with their temperature a few minutes earlier, the circadian rhythm itself does not cause the next minute’s state merely by its past condition.\nThey tend to improve fit dramatically and with statistically significant coefficients. We can see that the R2 for the model with no lags is 0.02 (Table 8 (b)). When we add the lag terms the R2 becomes 0.96 (Table 8 (a)) and all lag terms are significant (p-value &lt; 0.0001). Indeed, a dramatic improvement.\nMany of the remaining substantive coefficients collapse to implausibly small and insignificant values, occasionally taking on the wrong sign leading to inaccurate inference. The coefficient for the simulated variable for the model with no lags is 0.2881 and its p-value is &lt;0.0001 (Table 8 (b)). When we add the lag terms the coefficient becomes 30 times smaller at 0.0094 and its p-value is non significant at 0.1022 (Table 8 (a)).\n\nThis simulation clearly illustrates how incorporating a set of highly predictive lagged features can lead to a model with impressive predictive accuracy. However, it comes at the cost of interpretability, as it complicates our ability to “identify a small set of causal covariates”. Indeed, to use Achen’s terminology, if one is interested in inference and explanations, the model with the lag terms is outlandish.\nThis trade-off highlights a fundamental principle in statistical modelling — the distinction between prediction and inference: do we aim to predict or explain? Again, I refer to Efron’s paper9 for more as well as Shmueli10.\n9 See: Efron, Bradley. “Prediction, Estimation, and Attribution. International Statistical Review 88, no. S1 (2020): S28–59.This paper further explores the difference between prediction and inference (or more precisely, a type of inference called attribution):”evidently there are a great many genes weakly correlated with prostate cancer, which can be combined in different combinations to give near-perfect predictions. This is an advantage if prediction is the only goal, but a disadvantage as far as attribution is concerned. Traditional methods of attribution operate differently, striving as in Table 1 to identify a small set of causal covariates (even if strict causality cannot be inferred)“..10 Shmueli, Galit. “To Explain or to Predict?” Statistical Science 25, no. 3 (August 2010): 289–310.\n\nWhat About xAI?\nThe distinction between explanation and prediction does not vanish simply because we alter the approach. In the field of explainable AI (xAI)11, machine learning models generate predictions, but their internal decision-making processes often remain opaque. To address this, post-hoc explanation methods such as SHAP values (Shapley Additive Explanations) and LIME (Local Interpretable Model-agnostic Explanations) are applied to explain how these models arrive at their conclusions.\n11 https://en.wikipedia.org/wiki/Explainable_artificial_intelligence12 Researchers have developed techniques to introduce uncertainty measures into ML models, namely, quantile predictions and, more recently, conformal predictions. While conformal prediction is a promising methods, ML models remain fundamentally black-box predictors compared to traditional statistical approaches that explicitly quantify uncertainty.One key difference in machine learning is the absence of the inferential machinery that both frequentist and Bayesian statistical approaches provide. As a result, we cannot properly formulate formal conclusions in the same way that frequentist and Bayesian inference allows. More precisely, in machine learning we lack the ability to quantify uncertainty in our findings12.\n\n\nCode\n# Take a subset of the data\nset.seed(321)\nsim &lt;- yyc_temp %&gt;%\n  drop_na() %&gt;% \n  mutate(simulated = 0.05 * temp + rnorm(n(), sd = 5))\n  \nsim &lt;- sim %&gt;%\n  mutate(time_num = as.numeric(time),\n         hour     = as.numeric(format(time, \"%H\")),\n         doy      = as.numeric(format(time, \"%j\"))\n  )\n\n# Generate lags as before\nsim &lt;- sim %&gt;% \n  mutate(temp_lag_3  = lag(temp, 3),\n         temp_lag_6  = lag(temp, 6),\n         temp_lag_24 = lag(temp, 24),\n         temp_lag_27 = lag(temp, 27),\n         temp_lag_45 = lag(temp, 45),\n  )\n\n## Remove NAs\nsim &lt;- sim %&gt;% \n  drop_na()\n\n## Libraries\nlibrary(tidymodels)\nlibrary(ranger)\nlibrary(treeshap)\nlibrary(shapviz)\nlibrary(kernelshap)\n\n## Data split\nset.seed(123)\ndata_split &lt;- initial_split(sim, prop = 0.8)\ntrain_data &lt;- training(data_split)\ntest_data &lt;- testing(data_split)\n\n## Model recipe\nrf_model &lt;- rand_forest(\n  mode  = \"regression\",\n  trees = tune(),\n  mtry  = 3,\n  min_n = tune()) %&gt;%\n  set_engine(\"ranger\",\n             num.threads = parallel::detectCores() - 1)\n\nrf_recipe &lt;- recipe(temp ~ time_num + simulated + temp_lag_3 + temp_lag_6 + temp_lag_24 + temp_lag_27 + temp_lag_45, data = train_data)\n\n## Assemble workflow\nrf_workflow &lt;- workflow() %&gt;%\n  add_model(rf_model) %&gt;%\n  add_recipe(rf_recipe)\n\n## Tuning parameters\nrf_grid &lt;- grid_regular(\n  trees(range = c(100, 120)), #do 100-200\n  min_n(range = c(10, 12)),\n  levels = 2)\n\nset.seed(123)\nrf_res &lt;- tune_grid(\n  rf_workflow,\n  resamples = bootstraps(train_data, times = 10),\n  grid      = rf_grid,\n  metrics   = metric_set(rmse))\n\n# Select best parameters & prepare final workflow\nbest_params &lt;- select_best(rf_res)\nfinal_rf_workflow &lt;- finalize_workflow(rf_workflow, best_params)\n\n# Fit chosen model:\nrf_fit &lt;- fit(final_rf_workflow, data = train_data)\n\n\n##SHAP\nset.seed(123)\nxvars &lt;- c(\"time_num\", \"simulated\", \"temp_lag_3\", \"temp_lag_6\", \"temp_lag_24\", \"temp_lag_27\", \"temp_lag_45\")\n\nX_explain &lt;- train_data[sample(1:nrow(train_data), 1000), xvars]\nX_background &lt;- train_data[sample(1:nrow(train_data), 200), ]\n\n## About 45min\nshap_values &lt;- permshap(rf_fit, X = X_explain, bg_X = X_background)\nshap_values &lt;- shapviz(shap_values)\n\n## Save results\nsaveRDS(shap_values, file = \"posts/2024-07-28-yyc-temperatures/Data/shap_values.rds\")\nsaveRDS(rf_fit, file = \"posts/2024-07-28-yyc-temperatures/Data/rf_fit.rds\", compress = \"xz\")\n\n\n\n\nCode\nlibrary(shapviz)\n\n## Read orchestrated data:\nshap_values &lt;- readRDS(\"Data/shap_values.rds\")\n\n## Plot SHAP\ngg1 &lt;- sv_importance(shap_values, show_numbers = TRUE)\ngg2 &lt;- sv_importance(shap_values, kind = \"bee\")\n# sv_dependence(shap_values, v = xvars)\n\ngg1+gg2\n\n\n\n\n\n\n\n\nFigure 22: SHAP importance plots for the a random forest model on the hourly data that includes lags of temperature and a simulated variable weakly correlated with temperatures. Notice the low importance of the simulated variable.\n\n\n\n\n\n\nOne can see that the SHAP importance for the simulated variable is the lowest of any feature. Just like with the model in Table 8 (a), fooled by the impressive accuracy of the random forest model, practitioners may be inclined to throw away the simulated variable altogether as uninformative. If the goal is to understand, we just lost the opportunity to do so."
  },
  {
    "objectID": "posts/2024-07-28-yyc-temperatures/index.html#interaction",
    "href": "posts/2024-07-28-yyc-temperatures/index.html#interaction",
    "title": "Exploring Changes in Temperatures in Calgary",
    "section": "Interaction",
    "text": "Interaction\nI expect that an interaction between month and year is sensible. In other words, I would expect that some months experience higher temperature changes over the years than other months. We call this an interaction, whereby the effect of year on temperature depends on the month. Before we fit models with interactions, let’s explore visually whether the data supports this.\n\n\nCode\n## Aggregate data to explore effect of year. Keep year continuous.\nyyc_monthly &lt;- yyc %&gt;%\n  mutate(year      = as.numeric(year(time))) %&gt;% \n  mutate(month     = as.factor(as.character(month(time, label = TRUE, abbr = TRUE)))) %&gt;% \n  group_by(year, month) %&gt;% \n  summarise(temp_mean = mean(temp, na.rm = TRUE),\n            temp_min = min(temp, na.rm = TRUE),\n            temp_p10 = quantile(temp, probs = 0.10, na.rm = TRUE),\n            temp_p25 = quantile(temp, probs = 0.25, na.rm = TRUE),\n            temp_p50 = quantile(temp, probs = 0.50, na.rm = TRUE),\n            temp_p75 = quantile(temp, probs = 0.75, na.rm = TRUE),\n            temp_p90 = quantile(temp, probs = 0.90, na.rm = TRUE),\n            temp_max = max(temp, na.rm = TRUE)\n            ) %&gt;% \n  ungroup() %&gt;% \n  mutate(date = ymd(paste0(as.character(year), \"-\", as.character(month), \"-\", \"01\"))) %&gt;% \n  mutate(month = fct_relevel(month, \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")) %&gt;% \n  select(date, everything()) \n\n\n\n\nCode\nyyc_monthly %&gt;% \n  ggplot(aes(x = year, y = temp_mean, color = month, label = month)) +\n  stat_plsmo() +\n  labs(title   = \"\",\n       x       = \"\",\n       y       = \"Mean Monthly Temperature (C°)\",\n       color   = \"\",\n       caption = \"Data: Environment and Climate Change Canada (2024). Historical Climate Data. Government of Canada.\"\n       ) +\n  scale_color_manual(values = cyclic_colors) +\n  theme(plot.subtitle = element_markdown())\n\n\n\n\n\n\n\n\nFigure 23: Seasonal plot to highlight potential interactions. We see some visual evidence for interaction in some months, namely, January."
  },
  {
    "objectID": "posts/2024-07-28-yyc-temperatures/index.html#inferential-model-1",
    "href": "posts/2024-07-28-yyc-temperatures/index.html#inferential-model-1",
    "title": "Exploring Changes in Temperatures in Calgary",
    "section": "Inferential Model 1",
    "text": "Inferential Model 1\n\n\nCode\n## Model\ndd &lt;- datadist(yyc_monthly)  \noptions(datadist = \"dd\")\n\nf1 &lt;- ols(temp_mean ~ year*month, data = yyc_monthly, x = TRUE, y = TRUE)\n\n## Use robust HC3 errors (\"Efron\")\nf1_hc3 &lt;- robcov(f1, method = \"efron\")\n\n\n\n\nCode\noptions(prType = \"html\")\nprint(f1, coefs = FALSE, table = FALSE, digits = 2)\n\n\n\n\n\n\nLinear Regression Model\n\nols(formula = temp_mean ~ year * month, data = yyc_monthly, x = TRUE, \n    y = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\nModel Likelihood\nRatio Test\nDiscrimination\nIndexes\n\n\n\n\nObs 864\nLR χ2 2040.63\nR2 0.906\n\n\nσ 2.92\nd.f. 23\nR2adj 0.903\n\n\nd.f. 840\nPr(&gt;χ2) 0.0000\ng 10.230\n\n\n\n\n\n\nResiduals\n\n   Min     1Q Median     3Q    Max \n-13.87  -1.52   0.07   1.72   9.21 \n\n\n\n\n\nTable 5: Table of model fitting metrics.\n\n\n\n\n\n\nCode\n(an &lt;- anova(f1))\n\n\n\n\n\n\n\n\nAnalysis of Variance for temp_mean\n\n\n\nd.f.\nPartial SS\nMS\nF\nP\n\n\n\n\nyear (Factor+Higher Order Factors)\n12\n532.4892\n44.374099\n5.19\n&lt;0.0001\n\n\n All Interactions\n11\n243.2190\n22.110821\n2.59\n0.0031\n\n\nmonth (Factor+Higher Order Factors)\n22\n68700.5477\n3122.752168\n365.40\n&lt;0.0001\n\n\n All Interactions\n11\n243.2190\n22.110821\n2.59\n0.0031\n\n\nyear × month (Factor+Higher Order Factors)\n11\n243.2190\n22.110821\n2.59\n0.0031\n\n\nREGRESSION\n23\n68989.8179\n2999.557298\n350.99\n&lt;0.0001\n\n\nERROR\n840\n7178.6555\n8.546018\n\n\n\n\n\n\n\n\nTable 6: ANOVA table\n\n\n\n\n\n\n\nCode\nset.seed(123)\nval &lt;- validate(f1, B = 500)\nhtml(val)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndex\nOriginal\nSample\nTraining\nSample\nTest\nSample\nOptimism\nCorrected\nIndex\nSuccessful\nResamples\n\n\n\n\n\nR2\n\n0.9058\n0.908\n0.903\n0.0049\n0.9008\n500\n\n\nMSE\n8.3086\n8.1163\n8.5474\n-0.431\n8.7397\n500\n\n\ng\n10.2296\n10.2418\n10.2191\n0.0227\n10.2069\n500\n\n\nIntercept\n0\n0\n0.015\n-0.015\n0.015\n500\n\n\nSlope\n1\n1\n0.9974\n0.0026\n0.9974\n500\n\n\n\n\n\n\nTable 7: Table of model validation metrics based on the optimism bootstrap. Optimism bootstrap allows us to use the whole data for estimation and avoid splitting the data into training and testing13 . The bootstrap is then used to adjust the naive estimate of model accuracy. Here, the amount of optimism negligible for any of the metrics. It should not come as a surprise that there is no overfitting because the ratio sample size to the number of candidate variables is very large.\n\n13 See: https://hbiostat.org/rmsc/validate#sec-val-bootval\n\n\n\n\nCode\n## Diagnostic\ndx &lt;- yyc_monthly %&gt;% \n  bind_cols(yhat  = predict(f1)) %&gt;% \n  bind_cols(resid = resid(f1)) %&gt;% \n  mutate(month = fct_relevel(month, \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \n                             \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"))\n\ndx %&gt;% \n  ggplot(aes(x = yhat, y = resid, color = month)) + \n  geom_point(alpha = 0.8) +\n  scale_color_manual(values = cyclic_colors) +\n  scale_x_continuous(limits = c(-20, 20)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"black\") +\n  labs(x                 = \"Predicted Mean Hourly Temperature (C°)\",\n       y                 = \"Residuals\",\n       color             = \"\",\n       title             = \"\",\n       caption           = \"Data: Environment and Climate Change Canada (2024). Historical Climate Data. Government of Canada.\") +\n  coord_cartesian(expand = FALSE) +\n  theme(legend.position = \"right\") +\n  theme(plot.subtitle = element_markdown())\n\ndx %&gt;% \n  ggplot(aes(sample = resid)) + \n  stat_qq(alpha = 0.5) +\n  stat_qq_line() +\n  labs(x = \"Theoretical\",\n       y = \"Sample\")\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Constant variance and zero mean assumption\n\n\n\n\n\n\n\n\n\n\n\n(b) Normality\n\n\n\n\n\n\n\nFigure 24: Left: zero mean errors and constant variance checks. The errors are visibly heteroskedastic with cold months showing much more variation than warm months. Right: quantile-quantile plot for normality of errors.\n\n\n\n\nLet’s look at the output:\n\nFrom Table 5, the R2 is 0.9. This means that 90% of the changes in monthly temperatures between 1954 and 2024 are explained by this model 14.\nFrom Table 5, the likelihood ratio \\(\\chi^2\\) is 2041 and the p-value associated with it is &lt;0.001. The test statistic tells us that at least one of the coefficients in the model is significantly different from 0 and, therefore, that overall the model is useful.\nFrom Table 5, \\(g\\) stands for \\(g-index\\) and represents the Gini mean difference, a measure of dispersion 15. A value of 10.2 suggests that the typical difference in mean monthly temperatures between any two predictions is 10 degrees C°. This is useful to benchmark among multiple models. One could also compare the \\(g-index\\) from the model above to the Gini mean difference on the raw data which is 11.\nThe AIC (Akaike information creterion - not shown) is 4331. This metric could be used to benchmark across multiple models. Provided all the regression assumptions are met, minimizing AIC is asymptotically equivalent to minimize MSE via leave-one-out cross-validation.\nFrom Table 6, the model has an interaction between year and month. In other words, we are allowing the effect of month on temperature to vary by year. Doing so allows one to explore the possibility that temperature increases over the years may be experienced in some months more than others. From the ANOVA table, we see that with a p-value of 0.003, overall this interaction is statistically significant.\nThe year variable is treated as a continuous variable. Its coefficient (not shown) tells us that every one year increment is associated with an increase in temperatures of 0.1. This may appear small, but over 70 years it is: 70 * 0.1 = 7 16. The p-value associated with the year coefficient is not significant at the 5% level, but it is at a more liberal 10%.\nMoving on to the diagnostic plot (Figure 24), while the error are not overly problematic (QQ plot on the right), we do seem to have a problem with the constant variance assumption: colder temperatures have a higher variance than warmer temperatures and this is quite evident in the graph. We therefore need to either utilize some form of robust standard errors or use a different regression method. For now, I will use a type of robust standard errors called Newey-West17. This is a type heteroskedasticity and autocorrelation consistent (HAC) covariance estimation.\nFrom the validation table in Table 7 there is no signs of overfitting or regression to the mean.\n\n14 There is little value in reporting the R2 statistic for a test set to assess overfitting because we are only using 23 degrees of freedom with a sample size of 864. Even though the sample size is large relative to the number of predictors, I feel it’s more sensible to utilize the whole data for estimation purposes. To assess how the model generalizes to unseen data, the optimism bootstrap will be utilized.15 The Gini mean difference is significantly more interpretable than, say, standard deviation and does not require to choose a measure of central tendency (the standard deviation require one to use the mean). Besides being a robust measure, the Gini mean difference is 0.98 as efficient as the standard deviation if the distribution were Gaussian.16 One aspect worth highlighting is that since we have a model with a significant interaction, evaluating this main effect is of limited interest (because we can evaluate the effect of year on temperatures for each month individually). This is in the same ballpark as what we observed in Figure 6.17 See: Newey WK, West KD (1994). “Automatic Lag Selection in Covariance Matrix Estimation.” Review of Economic Studies, 61, 631–653 and Zeileis, A. (2004). Econometric Computing with HC and HAC Covariance Matrix Estimators. Journal of Statistical Software, 11(10), 1–17.\n\nUsing Robust Standard Errors\nLet us now apply the heteroskedasticity and autocorrelation consistent (HAC) standard errors using the Newey-West method and compare the results side by side:\n\nCode\n## Refit using conventional lm\nf2 &lt;- lm(temp_mean ~ year*month, data = yyc_monthly)\n\n\n# Table 1 using conventional s.e.\nf2 %&gt;% \n  broom::tidy() %&gt;% \n  kable(digits = c(0, 2, 2, 2, 3)) %&gt;% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"responsive\"), font_size = 14, full_width = TRUE) %&gt;% \n  column_spec(3, bold = TRUE)\n## Table 2: using Newey-West cov estimator. Let it estimate lags\nf2_nw &lt;- coeftest(f2, df = Inf, vcov = NeweyWest)\nf2_nw_ci &lt;- confint(f2, vcov = NeweyWest)\n\nf2_nw %&gt;% \n  broom::tidy() %&gt;% \n  kable(digits = c(0, 2, 2, 2, 3)) %&gt;% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"responsive\"), font_size = 14, full_width = TRUE) %&gt;% \n  column_spec(3, bold = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    -216.37 \n    32.97 \n    -6.56 \n    0.000 \n  \n  \n    year \n    0.10 \n    0.02 \n    6.31 \n    0.000 \n  \n  \n    monthFeb \n    190.16 \n    46.62 \n    4.08 \n    0.000 \n  \n  \n    monthMar \n    146.71 \n    46.62 \n    3.15 \n    0.002 \n  \n  \n    monthApr \n    165.51 \n    46.62 \n    3.55 \n    0.000 \n  \n  \n    monthMay \n    191.40 \n    46.62 \n    4.11 \n    0.000 \n  \n  \n    monthJun \n    200.89 \n    46.62 \n    4.31 \n    0.000 \n  \n  \n    monthJul \n    184.12 \n    46.62 \n    3.95 \n    0.000 \n  \n  \n    monthAug \n    181.70 \n    46.62 \n    3.90 \n    0.000 \n  \n  \n    monthSep \n    158.96 \n    46.62 \n    3.41 \n    0.001 \n  \n  \n    monthOct \n    235.61 \n    46.62 \n    5.05 \n    0.000 \n  \n  \n    monthNov \n    166.29 \n    46.62 \n    3.57 \n    0.000 \n  \n  \n    monthDec \n    162.37 \n    46.62 \n    3.48 \n    0.001 \n  \n  \n    year:monthFeb \n    -0.09 \n    0.02 \n    -4.03 \n    0.000 \n  \n  \n    year:monthMar \n    -0.07 \n    0.02 \n    -3.02 \n    0.003 \n  \n  \n    year:monthApr \n    -0.08 \n    0.02 \n    -3.28 \n    0.001 \n  \n  \n    year:monthMay \n    -0.09 \n    0.02 \n    -3.71 \n    0.000 \n  \n  \n    year:monthJun \n    -0.09 \n    0.02 \n    -3.83 \n    0.000 \n  \n  \n    year:monthJul \n    -0.08 \n    0.02 \n    -3.41 \n    0.001 \n  \n  \n    year:monthAug \n    -0.08 \n    0.02 \n    -3.38 \n    0.001 \n  \n  \n    year:monthSep \n    -0.07 \n    0.02 \n    -2.99 \n    0.003 \n  \n  \n    year:monthOct \n    -0.11 \n    0.02 \n    -4.76 \n    0.000 \n  \n  \n    year:monthNov \n    -0.08 \n    0.02 \n    -3.44 \n    0.001 \n  \n  \n    year:monthDec \n    -0.08 \n    0.02 \n    -3.45 \n    0.001 \n  \n\n\n\n\n\n\n(a) Table of regression coefficients using the conventional covariance estimator.\n\n\n\n\n\n\n\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    -216.37 \n    45.11 \n    -4.80 \n    0.000 \n  \n  \n    year \n    0.10 \n    0.02 \n    4.64 \n    0.000 \n  \n  \n    monthFeb \n    190.16 \n    69.44 \n    2.74 \n    0.006 \n  \n  \n    monthMar \n    146.71 \n    58.66 \n    2.50 \n    0.012 \n  \n  \n    monthApr \n    165.51 \n    48.83 \n    3.39 \n    0.001 \n  \n  \n    monthMay \n    191.40 \n    48.08 \n    3.98 \n    0.000 \n  \n  \n    monthJun \n    200.89 \n    43.63 \n    4.60 \n    0.000 \n  \n  \n    monthJul \n    184.12 \n    46.25 \n    3.98 \n    0.000 \n  \n  \n    monthAug \n    181.70 \n    45.43 \n    4.00 \n    0.000 \n  \n  \n    monthSep \n    158.96 \n    49.96 \n    3.18 \n    0.001 \n  \n  \n    monthOct \n    235.61 \n    51.03 \n    4.62 \n    0.000 \n  \n  \n    monthNov \n    166.29 \n    63.61 \n    2.61 \n    0.009 \n  \n  \n    monthDec \n    162.37 \n    72.85 \n    2.23 \n    0.026 \n  \n  \n    year:monthFeb \n    -0.09 \n    0.03 \n    -2.71 \n    0.007 \n  \n  \n    year:monthMar \n    -0.07 \n    0.03 \n    -2.41 \n    0.016 \n  \n  \n    year:monthApr \n    -0.08 \n    0.02 \n    -3.15 \n    0.002 \n  \n  \n    year:monthMay \n    -0.09 \n    0.02 \n    -3.61 \n    0.000 \n  \n  \n    year:monthJun \n    -0.09 \n    0.02 \n    -4.11 \n    0.000 \n  \n  \n    year:monthJul \n    -0.08 \n    0.02 \n    -3.45 \n    0.001 \n  \n  \n    year:monthAug \n    -0.08 \n    0.02 \n    -3.48 \n    0.000 \n  \n  \n    year:monthSep \n    -0.07 \n    0.03 \n    -2.81 \n    0.005 \n  \n  \n    year:monthOct \n    -0.11 \n    0.03 \n    -4.37 \n    0.000 \n  \n  \n    year:monthNov \n    -0.08 \n    0.03 \n    -2.53 \n    0.011 \n  \n  \n    year:monthDec \n    -0.08 \n    0.04 \n    -2.21 \n    0.027 \n  \n\n\n\n\n\n\n(b) Table of regression coefficients using the Newey-West covariance estimator. While the differences appear small and the conclusion hardly change, it’s easier to appreciate the difference through a partial effect like the one in Figure 27.\n\n\n\n\n\n\n\nTable 8\n\n\n\n\n\n\nCode\n## Compute Newey-West cov\nrobust_cov &lt;- vcovHAC(f2, type = \"NeweyWest\")\nrobust_anova &lt;- Anova(f2, vcov = robust_cov, type = 3)\n  \n## ANOVA\noptions(contrasts = c(\"contr.sum\", \"contr.poly\"))\nkable(broom::tidy(Anova(f2, vcov = robust_cov, type = 3)), digits = c(0, 0, 2, 3))\noptions(contrasts = c(\"contr.treatment\", \"contr.poly\"))\n\n\n\n\n\n\n\n\nterm\ndf\nstatistic\np.value\n\n\n\n\n(Intercept)\n1\n22.33\n0.000\n\n\nyear\n1\n20.87\n0.000\n\n\nmonth\n11\n2.21\n0.012\n\n\nyear:month\n11\n1.94\n0.031\n\n\nResiduals\n840\nNA\nNA\n\n\n\n\n\n\nTable 9: ANOVA table using the Newey-West robust standard errors. The ANOVA table shows the significance of the month x year interaction term. Because this interaction is significant, the main effects are of limited interest.\n\n\n\n\n\n\n\nInfluential Years-Months\nNext let’s take a look at influential observations. We first plot the Cook’s distance, followed by the DFBETAS. Cook’s distance is arguably slightly more suitable for predictions, whereas DFBETAS are more sutiable for explanatory modelling18. Nonetheless, the two approaches should give very similar results.\n18 see here\n\nCode\n# Prepare the data frame with Cook's Distance and combine it with the original dataset\ncooks_d &lt;- cooks.distance(f2)\nthreshold &lt;- 0.01\n# threshold &lt;- 4/(nrow(yyc_monthly) - (nrow(yyc_monthly) - df.residual(f2) - 1) - 1)\n\ncooks &lt;- yyc_monthly %&gt;%\n  mutate(Observation    = row_number(),\n         Cooks_Distance = cooks.distance(f2),\n         Influential    = cooks_d &gt; threshold)\n\n# Plot\ncooks %&gt;% \n  ggplot(aes(x = Observation, y = Cooks_Distance)) +\n  geom_hline(yintercept = 0.01, linetype = \"dashed\", color = \"firebrick2\") + \n  geom_point(aes(color = Influential), size = 2, show.legend = FALSE, alpha = 0.8) + \n  geom_text_repel(data = cooks %&gt;% filter(Influential),\n                  aes(label = paste(year, month, sep = \" \")),\n                  hjust = -0.3, vjust = -0.3, family = \"Roboto Condensed\", size = 3.4) +\n  scale_color_manual(values = c(\"TRUE\" = \"firebrick3\", \"FALSE\" = \"grey60\")) +\n  labs(title = \"Cook's Distance\",\n       x     = \"Observation\",\n       y     = \"Cook's Distance\",\n       caption = \"Data: Environment and Climate Change Canada (2024). Historical Climate Data. Government of Canada.\") +\n  coord_cartesian(expand = TRUE)\n\n\n\n\n\n\n\n\nFigure 25: Plot of Cook’s distance. Cook’s Distance evaluates how much individual data points impact the overall regression model. Specifically, it is the product of two components: the first reflects the degree of outlyingness, and the second accounts for leverage (i.e. the extent to which an observation can affect a prediction). Points with higher values indicate a greater impact on the model’s coefficients and predictions. The treshold value of 0.01 is arbitrary as there is no consensus on what constitutes a sensible cutoff. All of the ifluential observations are in the cold months which is expected given cold months have the most variation (see Figure 24). However, they largely occurred in the 1955-1965 decade.\n\n\n\n\n\nCaptured in Calgary on May 16, 2023 at 10 AM, the temperature at the time was around 16°C — an unusual month according to the DFBETAS influence statistics — this photo highlights smoke from forest fires which occurred earlier than usual that year.\n\n\n\nCode\n## Influential observations\nw &lt;- which.influence(f1_hc3, cutoff = 0.20)\n\n## Only keep the interaction (because main effect are just repetitions of same influential obs)\nw &lt;- w[4]\nnam &lt;- names(w)\ninf &lt;- yyc_monthly %&gt;% \n  select(date, temp_mean)\n\n# Initialize an empty list to store results for each iteration\nresults_list &lt;- list()\n\n# Loop through and store the results\nfor (i in 1:length(nam)) {\n  influential_obs &lt;- inf[w[[i]], ]\n  influential_obs$effect &lt;- nam[i]\n  results_list[[i]] &lt;- influential_obs\n}\n\n# Combine all the results into a single data frame\ntmp1 &lt;- do.call(rbind, results_list) %&gt;% \n  mutate(across(where(is.numeric), ~ round(., digits = 0))) %&gt;% \n  mutate(YearMonth = yearmonth(date)) %&gt;% \n  mutate(month = as.character(month(date, label = TRUE, abbr = TRUE))) %&gt;% \n  select(-effect, -date) %&gt;% \n  select(YearMonth, everything())\n\n\n# Term of comaprisons (average temperature in eahc month, regardless of year)\ntmp2 &lt;- yyc_monthly %&gt;% \n  group_by(month) %&gt;% \n  summarise(month_mean = round(mean(temp_mean, na.rm = TRUE), 0)) %&gt;% \n  ungroup() %&gt;% \n  mutate(month = as.character(month))\n\n\n# Merge\ntmp1 %&gt;% \n  left_join(tmp2, by = \"month\") %&gt;% \n  select(-month) %&gt;% \n  select(`Year/Month` = YearMonth,\n         `Mean C°` = temp_mean,\n         `Typical C° in Month` = month_mean) %&gt;% \n  kable() %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"responsive\"), full_width = F)\n\n\n\n\n\n\n\n\n\nYear/Month\nMean C°\nTypical C° in Month\n\n\n\n\n1954 Apr\n-4\n4\n\n\n1954 Jan\n-19\n-8\n\n\n1954 Nov\n4\n-2\n\n\n1955 Dec\n-14\n-7\n\n\n1955 Jan\n-6\n-8\n\n\n1955 Nov\n-13\n-2\n\n\n1958 Jan\n-2\n-8\n\n\n1961 Jan\n-3\n-8\n\n\n1964 Dec\n-17\n-7\n\n\n1966 Jan\n-18\n-8\n\n\n1969 Jan\n-24\n-8\n\n\n2019 Feb\n-18\n-6\n\n\n2021 Dec\n-12\n-7\n\n\n2023 Dec\n1\n-7\n\n\n2023 May\n15\n10\n\n\n2024 Jan\n-9\n-8\n\n\n\n\n\n\n\n\n\nTable 10: Table of unusual months identified by the model, alongside the average monthly temperature across all years (e.g., the mean temperature for all Januarys, Februarys, etc., over the 70-year dataset) for comparison. These are based on the DFBETAS statistic with a cutoff of 0.20.\n\n\n\n\n\n\n\nRevisiting the Interaction\nNow that we have the model, let us take another look at the shape of the interaction between month and year, this time estimating it from the model and focusing on the months of December and January.\n\n\nCode\nPredict(f1_hc3, year, month = c(\"Dec\", \"Jan\")) %&gt;% \n  ungroup() %&gt;% \n  as.data.frame() %&gt;% \n  ggplot(aes(x = year, y = yhat, color = month)) +\n  geom_line(linewidth = 1.3) +\n  geom_ribbon(aes(ymin = lower, ymax=upper, fill = month), alpha = 0.2, color = NA, show.legend = FALSE) +\n  labs(title = \"\",\n       x     = \"\",\n       y     = \"Predicted Mean Hourly Temperature (C°)\",\n       color = \"\",\n       caption = \"Data: Environment and Climate Change Canada (2024). Historical Climate Data. Government of Canada.\") +\n  coord_cartesian(expand = TRUE)\n\n\n\n\n\n\n\n\nFigure 26: Interaction plot between month and year, focusing on January and December. The plot illustrates that the effect of year on temperature varies by month, with evidence suggesting a faster rate of temperature increase over the years in January compared to December or any other month (global test of interaction: p-value=0.03).\n\n\n\n\n\n\n\n\nPartial Effect Plot\n\n\nCode\n## The following creates a partial effect plot with both conventional and Newey-West s.e. to illustrate the differences visually\nyhat &lt;- Predict(f1,\n                year  = c(1960, 1970, 1980, 1990, 2000, 2010, 2020),\n                month = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")) %&gt;% \n  ungroup() %&gt;% \n  as.data.frame() %&gt;% \n  select(-yhat, -lower, -upper)\n\n\n## Apply Newey-West robust s.e.\nyhat &lt;- yhat %&gt;%\n  mutate(fit         = predict(f2, newdata = yhat),\n         model_mat   = model.matrix(f2, newdata = yhat)[1:n(), ],\n         se_nw       = sapply(1:n(), function(i) sqrt(as.numeric(model_mat[i, ] %*% NeweyWest(f2, prewhite = FALSE) %*% model_mat[i, ]))),\n         lower_ci_nw = fit - qnorm(0.975) * se_nw,\n         upper_ci_nw = fit + qnorm(0.975) * se_nw\n         ) %&gt;%\n  select(-model_mat, -starts_with(\"se\"))\n\n\n## For comparison, calculate conventional s.e.:\nyhat_conv &lt;- Predict(f1,\n                year  = c(1960, 1970, 1980, 1990, 2000, 2010, 2020),\n                month = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")) %&gt;% \n  ungroup() %&gt;% \n  as.data.frame() %&gt;% \n  select(-yhat)\n\n\n## Merge two s.e. types into single dataframe:\nyhat &lt;- yhat %&gt;% \n  full_join(yhat_conv, by = c(\"year\", \"month\"))\n\n\n## Make it long for graphing purposes:\nyhat_long &lt;- yhat %&gt;%\n  pivot_longer(cols          = c(lower_ci_nw, upper_ci_nw, lower, upper),\n               names_to      = c(\".value\", \"type\"),\n               names_pattern = \"(lower|upper)(_ci_nw|)\"\n               ) %&gt;%\n  mutate(type = case_when(type == \"_ci_nw\" ~ \"Newey-West\",\n                          type == \"\" ~ \"Conventional\"))\n\n\n## Animation graph:\nanimation &lt;- yhat_long %&gt;% \n  ggplot(aes(x = fit, y = month, color = as.factor(year))) +\n  geom_point(position = position_dodge(width = 0.8), size = 2.5, alpha = 1) +\n  geom_linerange(aes(xmin = lower, xmax = upper), position = position_dodge(width = 0.8), linewidth = 0.75) +\n  scale_color_viridis_d(option = \"B\", end = 0.9) +\n  transition_states(type,\n                    transition_length = 0.10,\n                    state_length = 0.5) +\n  scale_color_viridis_d(option = \"B\", end = 0.9) +\n  scale_x_continuous(limits = c(-20, 20)) +\n  labs(x                 = \"Predicted Mean Monthly Temperature (C°)\",\n       y                 = \"\",\n       color             = \"\",\n       title             = \"Partial Effect Plot\",\n       subtitle          = 'Type of standard errors: {closest_state}',\n       caption           = \"Data: Environment and Climate Change Canada (2024). Historical Climate Data. Government of Canada.\") +\n  coord_cartesian(expand = FALSE) +\n  theme(legend.position  = \"right\") +\n  theme(plot.subtitle    = element_markdown())\n\n\n## Assemble animation.\n## N.B. width, height and res are not specified inside animate because they will be inherited via chunk options. There is a bug in the linux implenetation of magick resulting in an error when renderer magick is used.\nanimate(animation,\n        dev      = \"ragg_png\",\n        bg       = \"transparent\",\n        bitsize  = 16,\n        renderer = gifski_renderer(), \n        fps      = 30, \n        duration = 6)\n\n\n\n\n\n\n\n\nFigure 27: Partial effect plot of monthly temperatures at the beginning of each decade with 95% confidence intervals. For illustrative purposes, both conventional and Newey-West robust standard errors are presented, with the Newey-West intervals being the appropriate method.\n\n\n\n\n\n\n\n\nUsing Permutation Tests\nPermutation tests allow us to test the effect of a variable without relying on the normality of error and other assumptions. Suppose that we have a single mean temperature for every year. Moreover, suppose that year has no systematic effect on temperature. That is, any observed trend in temperature is due to chance variation. Then, if the hypothesis holds, randomly re-assigning temperatures to years should not change the results.\nWe can do this by generating all permutation (n! = 864!), compute the F statistic for each, and see the proportion exceeding the observed F statistic. If this proportion is small, then we reject the hypothesis that temperatures are unrelated to year in favour of the hypothesis that temperatures are related to year19.\n19 For more on permutation tests, see: Wheeler, “Permutation tests for linear models in R”.20 I’ve only shown the p-value in the presence of an interaction, however, models without the interaction which were run but not shown in this analysis support this.In reality, there is limited value to performing a permutation test for this particular application: the p-value for the main effect of year using two types of robust standard errors are both highly significant20.\nIf, on the other hand, the p-value for an additive model of temperatures on month and year were close to a decision boundary, and as we saw, frequentist assumptions do not hold, then a permutation test would be justified.\n\n\nCode\n## Manually:\n# lmod &lt;- lm(temp_mean ~ year + month, data = yyc_monthly)\n# lms &lt;- summary(lmod)\n# lms$fstatistic\n# 1 - pf(lms$fstatistic[1], lms$fstatistic[2], lms$fstatistic[3])\n# nreps &lt;- 1000\n# set.seed(123)\n# fstats &lt;- numeric(nreps)\n# for(i in 1:nreps){\n#   lmods &lt;- lm(sample(temp_mean) ~ year + month, yyc_monthly)\n#   fstats[i] &lt;- summary(lmods)$fstat[1]\n#   }\n# mean(fstats &gt; lms$fstat[1])\n\n## Using library lmPerm. Suppress messages\ninvisible(capture.output(perm &lt;- lmPerm::lmp(temp_mean ~ year*month, data = yyc_monthly, perm = \"Prob\")))\n\nsummary_perm &lt;- summary(perm)\n\ntidy_df &lt;- data.frame(\n  Term       = rownames(summary_perm$coefficients),\n  Estimate   = summary_perm$coefficients[, \"Estimate\"],\n  Iterations = summary_perm$coefficients[, \"Iter\"],\n  p.value    = summary_perm$coefficients[, \"Pr(Prob)\"]\n)\n\n# Print nicely\ntidy_df %&gt;% \n  kable(digits = c(0, 2, 0, 4), row.names = FALSE) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"responsive\"))\n# Anova - insanely hard way to make it into a table\noptions(contrasts = c(\"contr.sum\", \"contr.poly\"))\n\nanova_text &lt;- capture.output(anova(perm))\nanova_clean &lt;- anova_text[str_detect(anova_text, \"^[A-Za-z0-9]\")]\nanova_list &lt;- str_split_fixed(anova_clean, \"\\\\s{2,}\", 6)\nanova_df &lt;- as.data.frame(anova_list, stringsAsFactors = FALSE)\ncolnames(anova_df) &lt;- c(\"Term\", \"Df\", \"R Sum Sq\", \"R Mean Sq\", \"x1\", \"x2\")\n\nanova_df &lt;- anova_df %&gt;% \n  separate(`R Mean Sq`, into = c(\"R Mean Sq\", \"Pr(Prob)\"), sep = \"&lt;\") %&gt;% \n  separate(`R Mean Sq`, into = c(\"R Mean Sq\", \"Iter\"), sep = \" \") %&gt;% \n  select(Term, df=Df, `R Sum Sq`, `R Mean Sq`, Iter, `Pr(Prob)`) %&gt;% \n  mutate(df = as.numeric(df)) %&gt;% \n  mutate(`R Sum Sq` = as.numeric(`R Sum Sq`)) %&gt;% \n  mutate(`R Mean Sq` = as.numeric(`R Mean Sq`)) %&gt;% \n  mutate(Iter = as.numeric(Iter)) %&gt;% \n  mutate(`Pr(Prob)` = str_replace(`Pr(Prob)`, \"([0-9\\\\.e-]+)\\\\s*\\\\*+\", \"&lt;\\\\1\")) %&gt;% \n  drop_na(df)\n\n# Print with kable for a clean table format\nkable(anova_df, digits = 3) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"responsive\"))\noptions(contrasts = c(\"contr.treatment\", \"contr.poly\"))\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\nIterations\np.value\n\n\n\n\n(Intercept)\n4.30\n5000\n0.0000\n\n\nyear\n0.03\n5000\n0.0000\n\n\nmonth1\n-12.63\n5000\n0.0000\n\n\nmonth2\n-10.45\n5000\n0.0000\n\n\nmonth3\n-6.73\n5000\n0.0000\n\n\nmonth4\n-0.26\n5000\n0.0000\n\n\nmonth5\n5.74\n5000\n0.0000\n\n\nmonth6\n9.80\n5000\n0.0000\n\n\nmonth7\n12.62\n5000\n0.0000\n\n\nmonth8\n11.66\n5000\n0.0000\n\n\nmonth9\n6.79\n5000\n0.0000\n\n\nmonth10\n1.06\n5000\n0.0000\n\n\nmonth11\n-6.56\n5000\n0.0000\n\n\nyear:month1\n0.08\n5000\n0.0000\n\n\nyear:month2\n-0.02\n5000\n0.0000\n\n\nyear:month3\n0.01\n51\n0.6667\n\n\nyear:month4\n0.00\n51\n1.0000\n\n\nyear:month5\n-0.01\n5000\n0.0048\n\n\nyear:month6\n-0.01\n5000\n0.0024\n\n\nyear:month7\n0.00\n5000\n0.0026\n\n\nyear:month8\n0.00\n56\n0.6429\n\n\nyear:month9\n0.01\n51\n1.0000\n\n\nyear:month10\n-0.03\n5000\n0.0000\n\n\nyear:month11\n0.00\n51\n1.0000\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\ndf\nR Sum Sq\nR Mean Sq\nIter\nPr(Prob)\n\n\n\n\nyear\n1\n289\n289.3\n5000\n&lt;2.2e-16\n\n\nmonth\n11\n68457\n6223.4\n5000\n&lt;2.2e-16\n\n\nyear:month\n11\n243\n22.1\n5000\n&lt;2.2e-16\n\n\nResiduals\n840\n7179\n8.5\nNA\nNA\n\n\n\n\n\n\n\n\n\nTable 11: Table of regression coefficients using a permutation test with an ANOVA table at the bottom. The ‘Iter’ columns show the number of iterations required to meet the stopping rule. The stopping rule is reached when the estimated standard deviation falls below 0.1 of the estimated p-value.\n\n\n\n\n\nAs we can see from the ANOVA table in Table 11, the conclusions do not change. In fact, the evidence supporting an interaction is even stronger here.\n\n\n\nSummary of Inferential Model 1\nI am not a climate scientist, and this analysis is conducted more for education than scientific inquiry. Therefore, the conclusions drawn from these data should be interpreted as exploratory and illustrative. If these data were analyzed by experts in climate science with the same results, then, the following is an example of the type of conclusions we may reach.\n\nThere is evidence that the effect of year on temperatures depends on month. In particular, the temperature increase in January is higher compared to other months, suggesting an accelerating warming trend during certain months of the year (global test of interaction p-value = 0.031). Future analyses may want to focus on how colder temperatures have evolved over the years.\nThe coefficient for year associated with the month of January as estimated through an ordinary least square model with robust standard error in which year is treated linearly is 0.10°C (90% CI: 0.07, 0.14). This means that, on average, temperatures each January rise by 0.10°C. While this effect size may appear small, it becomes important when considered over decades. For example, over 50 years, the average temperature in January increased by approximately 5°C (0.10 × 50).\nBy contrast, the coefficient for year associated with the month of July is 0.02°C (95% CI: -0.05, 0.1). This means that, on average, temperatures each July rise by 0.02°C. Over the span of 50 years, the average temperature in July increased by approximately 1°C.\nThe p-value associated with year is &lt;0.001, which is below the conventional significance threshold of 0.05. This indicates that the effect of year is statistically significant at the 5% level. In layman’s terms, the data is consistent with the hypothesis of an increase in temperature. Said another way, the observed trend is unlikely to be due to random chance under the assumptions of the model.\nThe use of a permutation test leads to the same conclusions.\nWe observed several unusual cold snaps especially in the two decades of 1950-1970. More recently, the years 2018 to 2023 have also experienced unusually cold temperatures in some months."
  },
  {
    "objectID": "posts/2024-07-28-yyc-temperatures/index.html#inferential-model-2",
    "href": "posts/2024-07-28-yyc-temperatures/index.html#inferential-model-2",
    "title": "Exploring Changes in Temperatures in Calgary",
    "section": "Inferential Model 2",
    "text": "Inferential Model 2\nIn the first inferential model, the variance of the residual was heavily heteroskedastic and we addressed this by utilizing the Newey-West robust standard errors. We now try a more complex semi-parametric modelling approach that makes no assumption about the distribution of the residuals: the proportional odd ordinal regression model, a type of cumulative probability model21 22 23. As the name implies, ordinal models work on the ranks of the observations, which enables us to treat the resulting inference in a manner that is somewhat similar to logistic regression. Key advantages of ordinal models are:\n21 see: Resources for Ordinal Regression Models.22 See: Harrell, Frank E. Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis. Springer Series in Statistics. Springer International Publishing, 2015.23 See: Liu, Qi, Bryan E. Shepherd, Chun Li, and Frank E. Harrell. “Modeling Continuous Response Variables Using Ordinal Regression: Modeling Continuous Response Variables Using Ordinal Regression.” Statistics in Medicine 36, no. 27 (November 30, 2017): 4316–35.\nRobust to extremes: Regression coefficient estimates are not affected by extreme values of the response variable.\nNo assumptions about distribution shape: The model encodes the empirical cumulative distribution in the intercepts, allowing the response distribution to be continuous, discontinuous, clumped, bimodal, or have flooring/ceiling effects.\nTransformation preserving: Applying transformation to the response variable does not affect regression coefficient estimates.\nHandling flooring and ceiling effects: Naturally accommodates clumping around zero (flooring), upper limits of detection (ceiling), or similar constraints.\nExpress probabilities naturally: Enables direct probabilistic interpretations, such as exceedance probabilities.\nRobust to model misspecifications: General assessments of effects are not compromised by model misspecifications.\n\n\n\n\n\n\n\nFigure 28: A Guide To Odds Ratios: What They Are and How To Communicate Them Clearly by David Spiegelhalter\n\n\n\n\n\nCode\n## Aggregate data to explore effect of year. Keep year continuous.\nyyc_monthly &lt;- yyc %&gt;%\n  mutate(year      = as.numeric(year(time))) %&gt;% \n  mutate(month     = as.factor(as.character(month(time, label = TRUE, abbr = TRUE)))) %&gt;% \n  group_by(year, month) %&gt;% \n  summarise(temp_mean = mean(temp, na.rm = TRUE)) %&gt;% \n  ungroup() %&gt;% \n  mutate(date = ymd(paste0(as.character(year), \"-\", as.character(month), \"-\", \"01\"))) %&gt;% \n  mutate(month = fct_relevel(month, \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")) %&gt;% \n  select(date, everything()) \n\n\n\n\nChecking the Assumption\nThe main critical assumption for an ordinal model il the proportional odd assumption. If the assumption is met, we would expect parallelism in the logit-transformed cumulative distributions.\n\n\nCode\n## Proportional odds assumption\nlibrary(data.table)\nsetDT(yyc_monthly)\n\n## \nyyc_monthly &lt;- yyc_monthly %&gt;% \n  mutate(decade = as.factor(floor(year / 10) * 10))\n\nv &lt;- yyc_monthly[, ecdfSteps(temp_mean, extend=FALSE), by = .(month, decade)]\n\nv %&gt;% \n  ggplot(aes(x, qlogis(pmin(y, 0.99)), color = month, linetype = decade)) +\n  geom_step() +\n  labs(x = \"y\",\n       y = \"logit ECDF\",\n       color = \"\",\n       linetype = \"Decade of\")\n\nyyc_monthly &lt;- as.data.frame(yyc_monthly)\n\n\n\n\n\n\n\n\nFigure 29: Checking the proportional odds assumption. Parallelism of the logit-transformed cumulative distributions indicate good alignment with the assumption of the proportional odds model.\n\n\n\n\n\n\n\nCode\n## ORM model.\n## Some examples here: https://hbiostat.org/bbr/nonpar#kruskal-wallis-test\n\ndd &lt;- datadist(yyc_monthly); options(datadist = 'dd')\n\nf1 &lt;- orm(temp_mean ~ year*month,\n          data  = yyc_monthly, \n          scale = FALSE,\n          maxit = 24,\n          x     = TRUE,\n          y     = TRUE)\n\n## Prep estiamtes\nM &lt;- Mean(f1)\nqu &lt;- Quantile(f1)\np5  &lt;- function(x) qu(0.05, x)\np50 &lt;- function(x) qu(0.50, x)\np95 &lt;- function(x) qu(0.95, x)\n\n# Print model\noptions(prType = \"html\")\nprint(f1, coefs = FALSE, table = FALSE, digits = 2)\n\n\n\n\n\n\nLogistic (Proportional Odds) Ordinal Regression Model\n\norm(formula = temp_mean ~ year * month, data = yyc_monthly, x = TRUE, \n    y = TRUE, scale = FALSE, maxit = 24)\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Likelihood\nRatio Test\nDiscrimination\nIndexes\nRank Discrim.\nIndexes\n\n\n\n\nObs 864\nLR χ2 2122.58\nR2 0.914\nρ 0.949\n\n\nESS 864\nd.f. 23\nR223,864 0.912\nDxy 0.801\n\n\nDistinct Y 864\nPr(&gt;χ2) &lt;0.0001\nR223,864 0.912\n\n\n\nY0.5 4.824577\nScore χ2 1734.32\n|Pr(Y ≥ median)-½| 0.436\n\n\n\nmax |∂log L/∂β| 1×10-9\nPr(&gt;χ2) &lt;0.0001\n\n\n\n\n\n\n\n\n\n\nTable 12: Table of the ordinal regression model fitting metrics.\n\n\n\n\n\n\nCode\noptions(prType = \"html\")\n(an &lt;- anova(f1, test = 'LR'))\n\n\n\n\n\n\n\n\nLikelihood Ratio Statistics for temp_mean\n\n\n\nχ2\nd.f.\nP\n\n\n\n\nyear (Factor+Higher Order Factors)\n60.41\n12\n&lt;0.0001\n\n\n All Interactions\n18.73\n11\n0.0660\n\n\nmonth (Factor+Higher Order Factors)\n2119.73\n22\n&lt;0.0001\n\n\n All Interactions\n18.73\n11\n0.0660\n\n\nyear × month (Factor+Higher Order Factors)\n18.73\n11\n0.0660\n\n\nTOTAL\n2122.58\n23\n&lt;0.0001\n\n\n\n\n\n\nTable 13: ANOVA table from the ordinal regression model. We use the more powerful likelihood ratio test.\n\n\n\n\n\n\nCode\nset.seed(123)\nval &lt;- validate(f1, B = 300)\nhtml(val)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndex\nOriginal\nSample\nTraining\nSample\nTest\nSample\nOptimism\nCorrected\nIndex\nSuccessful\nResamples\n\n\n\n\nρ\n0.9494\n0.9506\n0.948\n0.0025\n0.9468\n300\n\n\nDxy\n0.8014\n0.8048\n0.7986\n0.0063\n0.7952\n300\n\n\nR2\n0.9143\n0.9166\n0.912\n0.0046\n0.9097\n300\n\n\nSlope\n1\n1\n0.9606\n0.0394\n0.9606\n300\n\n\ng\n8.121\n8.3009\n7.9697\n0.3312\n7.7898\n300\n\n\nMean |Pr(Y≥Y0.5)-0.5|\n0.4365\n0.439\n0.4362\n0.0027\n0.4337\n300\n\n\n\n\n\n\nTable 14: Table of model validation metrics based on the optimism bootstrap. Optimism bootstrap24 allows us to use the whole data for estimation and avoid splitting the data into training and testing. The bootstrap is then used to adjust the naive estimate of model accuracy. Here, the amount of optimism is negligible for any of the metrics. It should not come as a surprise that there is no overfitting because the ratio sample size to the number of candidate variables is very large.\n\n24 See: https://hbiostat.org/rmsc/validate#sec-val-bootval\n\n\n\nLet’s look at the output:\n\nThe are three R2 in the output in Table 12 (called pseudo R2). This is because there is no consensus among researchers on a single way to calculate it. Frank Harrell suggests using an R2 method that adjusts for ties. In the output above this is 0.912 (this is the third R2 - the two identical R2 imply there are no ties in the data as also reported in “Distinct Y”). This means that 91% of the changes in monthly temperatures between 1954 and 2024 are explained by this model.\nThe likelihood ratio \\(\\chi^2\\) in Table 12 is 2123 and the p-value associated with it is &lt;0.001. The test statistic tells us that at least one of the coefficients in the model is significantly different from 0 and, therefore, that overall the model is useful. Therefore, the model as a whole fits significantly better than an empty model.\nFrom Table 12, |Pr(Y&gt;=median)-0.5| = 0.436 is a measure of discriminative ability. If the model has no predictive power, the predicted probability for Y being greater than or equal to the median will be around 0.5.\nFrom Table 12, the Spearman’s ρ rank correlation between the linear predictor and Y is 0.949. Similarly, Somer’s \\(D_{xy}\\) is 0.801 and is also a measure of predictive discrimination which is related to concordance.\nFrom Table 13 we see a significant year effect, but slightly weaker evidence of an interaction effect between year and month compared to the ordinary least square model (Table 9).\nAs before, the model validation that uses the optimism bootstrap in Table 14 do not reveal any concern with respect to overfitting or regression to the mean.\n\n\n\n\nOdds Ratios\nLet’s now examine the model output to better illustrate how the ordinal model works. Below, we print the first 10 intercepts from the model (printing all intercepts would result in printing all temperatures since there are no ties):\n\n\nCode\noptions(prType = \"html\")\nprint(f1, digits = 3, table = FALSE, coefs=10, intercepts=TRUE)\n\n\n\n\n\n\nLogistic (Proportional Odds) Ordinal Regression Model\n\norm(formula = temp_mean ~ year * month, data = yyc_monthly, x = TRUE, \n    y = TRUE, scale = FALSE, maxit = 24)\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Likelihood\nRatio Test\nDiscrimination\nIndexes\nRank Discrim.\nIndexes\n\n\n\n\nObs 864\nLR χ2 2122.58\nR2 0.914\nρ 0.949\n\n\nESS 864\nd.f. 23\nR223,864 0.912\nDxy 0.801\n\n\nDistinct Y 864\nPr(&gt;χ2) &lt;0.0001\nR223,864 0.912\n\n\n\nY0.5 4.824577\nScore χ2 1734.32\n|Pr(Y ≥ median)-½| 0.436\n\n\n\nmax |∂log L/∂β| 1×10-9\nPr(&gt;χ2) &lt;0.0001\n\n\n\n\n\n\n\n\n\n\n\n\n\nβ\nS.E.\nWald Z\nPr(&gt;|Z|)\n\n\n\n\ny≥-19.0607527\n  -72.954\n 19.674\n-3.71\n0.0002\n\n\ny≥-18.4053763\n  -73.665\n 19.666\n-3.75\n0.0002\n\n\ny≥-17.9940476\n  -74.087\n 19.666\n-3.77\n0.0002\n\n\ny≥-17.9126344\n  -74.385\n 19.666\n-3.78\n0.0002\n\n\ny≥-16.6938172\n  -74.618\n 19.666\n-3.79\n0.0001\n\n\ny≥-16.6897321\n  -74.810\n 19.667\n-3.80\n0.0001\n\n\ny≥-16.3107527\n  -74.972\n 19.667\n-3.81\n0.0001\n\n\ny≥-15.6049731\n  -75.112\n 19.667\n-3.82\n0.0001\n\n\ny≥-15.0422043\n  -75.236\n 19.668\n-3.83\n0.0001\n\n\ny≥-14.9356183\n  -75.347\n 19.668\n-3.83\n0.0001\n\n\n…\n\n\n\n\n\n\n\n\n\n\nTable 15: First 10 intercepts from the ordinal regression model.\n\n\n\n\nThe tenth intercept y&gt;=-14.9356183 corresponds with the logit of Y (that is, mean monthly temperature) being at or above the -14.9 C° (i.e. as warm or warmer than -14.9 C°). The estimated β in this case is the logit of the empirical cumulative distribution function while holding month and year at their reference values of January and year zero, respectively. Because year zero is not very meaningful and the logit, while being useful mathematically, is not intuitive for direct interpretation, let’s evaluate the odds ratio for a 20 years increase from 2004 to 2024 in the month of January.\n\n\nCode\n## Referent month is already January\noptions(prType = \"html\")\nsummary(f1, year = c(2004, 2024), est.all = FALSE)\nsummary(f1, year = c(1974, 2024), est.all = FALSE)\n# k &lt;- contrast(f1, list(year = 2004), list(year = 2024), conf.type='profile')\n# print(k, fun=exp)\n\n\n\n\n\n\n\n\nEffects   Response: temp_mean\n\n\n\nLow\nHigh\nΔ\nEffect\nS.E.\nLower 0.95\nUpper 0.95\n\n\n\n\nyear\n2004\n2024\n20\n0.7855\n0.198\n0.3974\n1.174\n\n\n Odds Ratio\n2004\n2024\n20\n2.1940\n\n1.4880\n3.234\n\n\n\n\n\n\n\n\nEffects   Response: temp_mean\n\n\n\nLow\nHigh\nΔ\nEffect\nS.E.\nLower 0.95\nUpper 0.95\n\n\n\n\nyear\n1974\n2024\n50\n1.964\n0.4951\n0.9936\n2.934\n\n\n Odds Ratio\n1974\n2024\n50\n7.127\n\n2.7010\n18.810\n\n\n\n\n\n\nTable 16: These effect estimates represent the change in odds over two span of time. A 20-year increase from 2004 to 2024 results in the odds of observing higher temperatures increasing by a factor of 2.2 (95% CI: 1.5, 3.2). Over a 50-year span from 1974 to 2024, the odds rise by a factor of 7.1 (95% CI: 2.7, 18.8).\n\n\n\n\nLooking at the top of Table 16, we can say that the odds of January temperatures being as high or higher in 2024 increase by a factor of 2.2 compared to 2004 across any threshold of temperature. Because the 95% confidence interval ranges from 1.5 to 3.2, we can be 95% confident that the true estimate lies within this range. Furthermore, since the null value of 1 is not included in this range, this odds ratio is considered to be statistically significant at the 5% level.\nBy threshold of temperature we mean that by the proportional odds assumption, this odds ratio remains the same across all possible cutoffs of temperatures. Moreover, these odds ratios are over and above the effect of month of the year.\n\n\n\nMean and Percentiles\nOne of the many benefits of using an ordinal model is that since we have interval data, we can express changes in mean temperatures or even percentiles. Below I produce a partial effect plot of mean monthly temperatures similar to the one in Figure 27:\n\n\nCode\nPredict(f1, year = seq(1960, 2020, 10), month, fun = M) %&gt;%\n  as.data.frame() %&gt;% \n  ungroup() %&gt;% \n  mutate(year = as.factor(year)) %&gt;% \n  ggplot(aes(x = yhat, y = month, color = year)) +\n  geom_point(position = position_dodge(width = 0.8), size = 2.5, alpha = 1) +\n  geom_linerange(aes(xmin = lower, xmax = upper), position = position_dodge(width = 0.8), linewidth = 0.75) +\n  scale_color_viridis_d(option = \"B\", end = 0.9) +\n  scale_x_continuous(limits = c(-20, 20)) +\n  labs(x                 = \"Predicted Mean Monthly Temperature (C°)\",\n       y                 = \"\",\n       color             = \"\",\n       title             = \"\",\n       caption           = \"Data: Environment and Climate Change Canada (2024). Historical Climate Data. Government of Canada.\") +\n  theme(legend.position  = \"right\") +\n  theme(plot.subtitle    = element_markdown())\n\n\n\n\n\n\n\n\nFigure 30: Partial effect plot of predicted mean monthly temperatures at the beginning of each decade with 95% confidence intervals from an ordinal regression model.\n\n\n\n\n\n\nAnd similarly, we can do the same with the key percentiles. Notice that the confidence intervals are appropriately wider when estimating sample quantiles.\n\n\nCode\npp50 &lt;- Predict(f1, year = seq(1960, 2020, 10), month, fun = p50) %&gt;%\n  as.data.frame() %&gt;% \n  ungroup() %&gt;% \n  mutate(percentile = \"50\")\n\npp5 &lt;- Predict(f1, year = seq(1960, 2020, 10), month, fun = p5) %&gt;%\n  as.data.frame() %&gt;% \n  ungroup() %&gt;% \n  mutate(percentile = \"5\")\n\npp95 &lt;- Predict(f1, year = seq(1960, 2020, 10), month, fun = p95) %&gt;%\n  as.data.frame() %&gt;% \n  ungroup() %&gt;% \n  mutate(percentile = \"95\")\n\npp_all &lt;- pp50 %&gt;% \n  bind_rows(pp5) %&gt;% \n  bind_rows(pp95)\n\nanimation2 &lt;- pp_all %&gt;% \n  mutate(year = as.factor(year)) %&gt;% \n  ggplot(aes(x = yhat, y = month, color = year)) +\n  geom_point(position = position_dodge(width = 0.8), size = 2.5, alpha = 1) +\n  geom_linerange(aes(xmin = lower, xmax = upper), position = position_dodge(width = 0.8), linewidth = 0.75) +\n  transition_states(percentile,\n                    transition_length = 0.10,\n                    state_length = 0.5) +\n  scale_color_viridis_d(option = \"B\", end = 0.9) +\n  scale_x_continuous(limits = c(-25, 25)) +\n  labs(x                 = \"Predicted Median Monthly Temperature (C°)\",\n       y                 = \"\",\n       color             = \"\",\n       title             = \"\",\n       caption           = \"Data: Environment and Climate Change Canada (2024). Historical Climate Data. Government of Canada.\") +\n  theme(legend.position  = \"right\") +\n  theme(plot.subtitle    = element_markdown())\n\n\n## Assemble animation.\n## N.B. width, height and res are not specified inside animate because they will be inherited via chunk options. There is a bug in the linux implenetation of magick resulting in an error when renderer magick is used.\nanimate(animation2,\n        dev      = \"ragg_png\",\n        bg       = \"transparent\",\n        bitsize  = 16,\n        renderer = gifski_renderer(), \n        fps      = 30, \n        duration = 6)\n\n\n\n\n\n\n\n\nFigure 31: Partial effect plot animation of predicted 5th, 50th, 95th percentile monthly temperatures at the beginning of each decade with 95% confidence intervals from an ordinal regression model.\n\n\n\n\n\n\n\n\nExceedance Probabilities\nWhile odds ratios are mathematically and statistically powerful, they can be difficult to interpret—unless you’re a seasoned gambler. In contrast, exceedance probabilities provide a more intuitive and effective way to quantify the likelihood of a given month-year combination.\nIn the plots below, we evaluate the exeedance probabilities at the beginning of each decade separately for each month.\n\n\nCode\n## Create new dataframe for model predictions:\nnewdata &lt;- data.frame(expand.grid(month = factor(c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"), levels = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")),\n                      year   = seq(1960, 2020, by = 10)))\n\n\n## Create the function, predict \nd &lt;- ExProb(f1)\nlp &lt;- predict(f1, newdata = newdata)\nw &lt;- d(lp)\n\n\n## Cleanup and add lost info:\nexprob.yyc &lt;- w[[\"prob\"]] %&gt;% \n  as_tibble() %&gt;%\n  mutate(year  = as.factor(c(rep(1960, 12), rep(1970, 12), rep(1980, 12), rep(1990, 12), rep(2000, 12), rep(2010, 12), rep(2020, 12)))) %&gt;% \n  mutate(month = rep(factor(c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"), levels = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")), 7)) %&gt;% \n  select(year, month, everything()) %&gt;% \n  select(-contains(\"V\")) %&gt;% \n  pivot_longer(cols = starts_with(\"Prob\"), names_to = \"temp\", values_to = \"prob\") %&gt;% \n  mutate(temp = parse_number(temp))\n\n\n## Plot data:\nexprob.yyc %&gt;% \n  ## Filter\n  # filter(month == \"Jan\") %&gt;%\n  ggplot(aes(x = temp, y = prob, color = year, label = year)) +\n  geom_line(linewidth = 0.5) +\n  geom_labelpath(size = 2.4, hjust = 0.4, rich = TRUE) +\n  facet_wrap(vars(month), scales = \"free\", ncol = 4) + \n  scale_color_colorblind(expand = TRUE, drop = TRUE) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  labs(x                = \"Monthly Temperature (C°)\",\n       y                = \"P[Y\\u2265y] (probability of a given temperature or warmer)\",\n       color            = \"\",\n       caption          = \"Data: Environment and Climate Change Canada (2024). Historical Climate Data. Government of Canada.\") +\n  theme(legend.position = \"bottom\") +\n  theme(axis.title = element_markdown()) +\n  theme(panel.grid.minor = element_blank()) +\n  theme(plot.margin = unit(c(1, 0, 0, 0), \"cm\")) +\n  guides(color = guide_legend(nrow = 1))\n\n\n\n\n\n\n\n\nFigure 32: Exceedance probabilities for combinations of year-month from a proportional odds cumulative probability model. \n\n\n\n\n\n\nBecause these plots are quite dense, let’s focus on the month of January:\n\n\nCode\n## Interactive graph via plotly:\nlibrary(plotly)\n\ngg &lt;- exprob.yyc %&gt;% \n  ## Filter\n  filter(month %in% c(\"Jan\")) %&gt;%\n  mutate(tooltip_temp = round(temp, 0),\n         tooltip_prob = paste0(as.character(round(prob*100, 0)), \"%\")) %&gt;% \n  mutate(`Temperature C°` = paste0(\" \", tooltip_temp, \"\\nP[Temperature \\u2265 \", tooltip_temp, \"]: \", tooltip_prob, \"\\nYear: \", year)) %&gt;%\n  ggplot(aes(x = temp, y = prob, color = year, label = `Temperature C°`)) +\n  geom_line(linewidth = 0.6) +\n  scale_color_colorblind(expand = TRUE, drop = TRUE) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  coord_cartesian(xlim = c(-20, 10), expand = TRUE) +\n  labs(x                = \"Monthly Temperature (C°)\",\n       y                = \"P[Y\\u2265y] (probability of a given temperature or warmer)\",\n       color            = \"\",\n       caption          = \"Data: Environment and Climate Change Canada (2024). Historical Climate Data. Government of Canada.\") +\n  theme(legend.position = \"bottom\") +\n  theme(axis.title = element_markdown()) +\n  theme(panel.grid.minor = element_blank()) +\n  theme(plot.margin = unit(c(1, 0, 0, 0), \"cm\")) +\n  guides(color = guide_legend(nrow = 1))\n\nggplotly(gg, tooltip = c(\"label\"))\n\n\n\n\n\n\n\n\nFigure 33: Exceedance probabilities for combinations of year-month from a proportional odds cumulative probability model. The month of January is shown.\n\n\n\n\n\nAs an example, let’s evaluate the exeedance probabilities of a set of temperatures and month:\n\nThe probability of average January temperatures as warm or warmer than -10 C° was 35% in 1960 and jumped to 85% in 2020.\n\n\n\n\n\n\n\n\nUncertainty Communication with Natural Frequencies\n\n\n\nNumerous empirical studies across social, clinical, and climate research indicate that natural frequencies are more effective in communicating uncertainty, as they are more intuitively understood by broader audiences.25 26 Below we translate the example interpretation into natural frequencies:\n\nIn 1960, a -10°C or warmer January happened about 35 times per 100 years27.\nBy 2020, the frequency jumped to 85 times per 100 years.\n\n\n\n27 The idea behind “35 times per 100 years” is simply saying that, on average, if we had 100 years of data just like 1960, we’d expect 35 years in which January reached an average temperature of -10°C or warmer.26 Gigerenzer, G. (2014). Risk savvy: How to make good decisions. Penguin.25 Ahmed, Haroon, Gurudutt Naik, Hannah Willoughby, and Adrian G. K. Edwards. “Communicating Risk.” BMJ 344 (June 18, 2012): e3996."
  },
  {
    "objectID": "posts/2024-07-28-yyc-temperatures/index.html#ai-transparency-statement",
    "href": "posts/2024-07-28-yyc-temperatures/index.html#ai-transparency-statement",
    "title": "Exploring Changes in Temperatures in Calgary",
    "section": "AI Transparency Statement",
    "text": "AI Transparency Statement\nThe table outlines any use of artificial intelligence in this project. We restrict the definition of AI to mean any generative AI technology, such as large language models (LLM’s) or other algorithms that generate text, images, or data based on a prompt.\n\n\n\n\n\n\nAI Transparency Dimension\nAnswer\nNotes\n\n\n\n\nHas any text been generated using AI?\nNo\nThe text has not been generated using AI; it was created manually.\n\n\nHas any text been improved or corrected using AI?\nYes\nParts of the text were improved using an AI system to enhance grammar, flow, and style, in order to increase clarity.\n\n\nHas any image, illustration or audio been generated or modified using AI?\nNo\nNo images, illustrations, or audio have been generated or modified using AI.\n\n\nHas any analysis code been generated using AI?\nNo\nNo analysis code has been generated using AI.\n\n\nHas any analysis code been improved or corrected using AI?\nYes\nSome analysis code has been improved using AI. This may include code refactoring, corrections, enhancements to coding style, comments, code clarity, or additional of options for commands statements. The modifications were verified by a human expert.\n\n\nHas any methods of analysis been suggested using AI?\nNo\nNo methods of analysis have been suggested using AI.\n\n\nDo any analyses utilize AI technologies, such as Large Language Models, for tasks like analyzing, summarizing, or retrieving information from data?\nNo\nNone of the analyses utilize AI technologies. All analyses were conducted using traditional methods and manual processes without the assistance of AI tools or algorithms.\n\n\n\nNote: \n\n\n\n\n Modified from the original work by Kester Brewin."
  },
  {
    "objectID": "posts/2023-08-31-agreement-btw-instruments/index.html",
    "href": "posts/2023-08-31-agreement-btw-instruments/index.html",
    "title": "Agreement Between Instruments",
    "section": "",
    "text": "We sometimes need to evaluate whether a new instrument agrees with an existing one. Instrument is loosely meant as a measuring device such as a sensor or an online analyzer, but it could also be a person measuring or rating something. Therefore, we wish to quantify how much the two agree/disagree."
  },
  {
    "objectID": "posts/2023-08-31-agreement-btw-instruments/index.html#correlation-coefficients",
    "href": "posts/2023-08-31-agreement-btw-instruments/index.html#correlation-coefficients",
    "title": "Agreement Between Instruments",
    "section": "Correlation Coefficients",
    "text": "Correlation Coefficients\nThere exist correlation coefficients that have been developed for these tasks. However, a critique of using correlation coefficients is that: “a perfect correlation can result even when the measurements disagree by a factor of 10” (Harrell, 1987).\nThe intraclass correlation coefficient (ICC) is a common one used for these tasks. Others include Cohen’s kappa statistic, Concordance Correlation Coefficient.\n\n\nSee: https://hbiostat.org/bbr/obsvar"
  },
  {
    "objectID": "posts/2023-08-31-agreement-btw-instruments/index.html#bland-altman-plot",
    "href": "posts/2023-08-31-agreement-btw-instruments/index.html#bland-altman-plot",
    "title": "Agreement Between Instruments",
    "section": "Bland-Altman Plot",
    "text": "Bland-Altman Plot\nA sensible visualization of the agreement between two measurements has been proposed by Bland and Altman. This happens to be the same as Tukey mean-difference plot. Bland and Altman develop this method to address the same shortcoming already pointed out earlier, namely, that “a high correlation does not necessarily imply that there is good agreement between the two methods”.\n\n\nYou can read the original paper here. Bland and Altman wrote in the Clinical Chemistry Journal (63:10 (2017)): “We certainly were not the first people to recognize that correlation coefficients did not assess agreement but rather association. Notably, we quoted Westgard and Hunt, who wrote wisely in Clinical Chemistry in 1973, “The correlation coefficient is of no practical use in the statistical analysis of comparison data”.”\n\n\n\n\nA peak expiratory flow rate (PEFR). Credit: healthline\n\n\nHere we illustrate the method using the authors original data on peak expiratory flow rate (PEFR) measures via two instruments: pefr1 and pefr2 on 17 subjects.\n\n\nCode\n## Import data used by Bland and ALtman in original paper\nba &lt;- read_csv(\"Data/bland-altman.csv\")\n# ba &lt;- read_csv(\"posts/2023-08-31-agreement-btw-instruments/Data/bland-altman.csv\")\n\n\nba %&gt;% \n  kable(escape = T, format = \"html\", digits = 1, align=rep('c', 5) ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"responsive\", \"compact\"), full_width = F)\n\n\n\n\n\n\n\n\n\nsubject\npefr1\npefr2\n\n\n\n\n1\n494\n512\n\n\n2\n395\n430\n\n\n3\n516\n520\n\n\n4\n434\n428\n\n\n5\n476\n500\n\n\n6\n557\n600\n\n\n7\n413\n364\n\n\n8\n442\n380\n\n\n9\n650\n658\n\n\n10\n433\n445\n\n\n11\n417\n432\n\n\n12\n656\n626\n\n\n13\n267\n260\n\n\n14\n478\n477\n\n\n15\n178\n259\n\n\n16\n423\n350\n\n\n17\n427\n451\n\n\n\n\n\n\n\n\n\nTable 1: Original data on PEFR1 and PEFR2 measruments.\n\n\n\n\nFor completeness, we show the estimated conventional correlation coefficient (i.e. Pearson) and the scatterplot, knowing that this (i.e. correlation) is not an appropriate way to analyze these data:\n\n\nCode\ncor(ba$pefr1, ba$pefr2, method = \"pearson\")\n\n\n[1] 0.9432794\n\n\nAs the authors write in the paper: “the high correlation of 0.94 for our own data conceals considerable lack of agreement between the two instruments”.\nWe want to know by how much the new method is likely to differ from the old. Ideally, we want to have a sense of the value beyond which we would say the two instruments are not in agreement. This is more a business decision than a data one, and needs to consider whether the difference between the two measurements is large enough to lead to a different decision.\n\n\nStep 0\nThere is a zero step in which we need to ask the subject matter expert the following question: how different does the second measurement need to be from the first measurement in order for it to lead to a different decision with regard to its use? Do this without peeking at the data, as it would lead to a cognitive bias (anchoring).\n\n\n\nStep 1\n\n\nCode\nba %&gt;% \n  ggplot(aes(x=pefr2, y = pefr1)) + \n  geom_point(shape=1) +\n  geom_abline(intercept = 0, slope = 1, size=0.25)\n\n\n\n\n\n\n\n\n\nFigure 1: PEFR1 vs PEFR2. The first step is to examine one measurement vs. the other with a 1:1 line as a reference\n\n\n\n\nWe first plot the two methods against each other as shown in Figure 1. This is a good start, however, the points will be clustered around the identity line thus making it difficult to assess between-method differences.\n\n\n\nStep 2\nWe next plot the differences between the measurements (on the y-axis), against the mean of the individual differences. We then add reference lines for the grand-mean of the differences, and +/- 2 standard deviation of the grand-mean. This is shown in Table 2 and Figure 2\n\n\nCode\n## Prepare diff, mean diff, sd of diff\nba &lt;- ba %&gt;% \n  mutate(diff = pefr1-pefr2) %&gt;%\n  mutate(mean = rowMeans(select(., starts_with(\"pefr\")), na.rm = TRUE)) %&gt;% \n  mutate(diff_mean = mean(diff)) %&gt;% \n  mutate(diff_sd = sd(diff, na.rm = TRUE)) \n\nba %&gt;% \n  slice(1:3) %&gt;% \n  kable(escape = T, format = \"html\", digits = 1, align=rep('c', 5) ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"responsive\", \"compact\"), full_width = F) \n\n\n\n\n\n\n\n\n\nsubject\npefr1\npefr2\ndiff\nmean\ndiff_mean\ndiff_sd\n\n\n\n\n1\n494\n512\n-18\n503.0\n-2.1\n38.8\n\n\n2\n395\n430\n-35\n412.5\n-2.1\n38.8\n\n\n3\n516\n520\n-4\n518.0\n-2.1\n38.8\n\n\n\n\n\n\n\n\n\nTable 2: Table of differences\n\n\n\n\n\n\nCode\nba %&gt;% \n  ggplot(aes(x = mean, y = diff)) +\n  geom_point(shape = 1, size = 2) +\n  scale_y_continuous(breaks = seq(-90, 90, by=20)) +\n  geom_hline(yintercept = ba$diff_mean, size = 0.5, linetype=\"dotted\") +\n  geom_hline(yintercept = ba$diff_mean + ba$diff_sd*2, size = 0.25) +\n  geom_hline(yintercept = ba$diff_mean - ba$diff_sd*2, size = 0.25) +\n  geom_hline(yintercept = 0, size = 0.25) +\n  annotate(\"text\", y = ba$diff_mean-3.5, x = 620, family = \"Pragmata Pro Mono\", size = 3.6, label = \"Mean of differences\") +\n  annotate(\"text\", y = ba$diff_mean + ba$diff_sd*2 + 4, x = 620, face = \"italic\", size = 3.6, family = \"Pragmata Pro Mono\", label = \"Mean + 2 SD\") +\n  annotate(\"text\", y = ba$diff_mean - ba$diff_sd*2 - 4, x = 620, face = \"italic\", size = 3.6, family = \"Pragmata Pro Mono\", label = \"Mean - 2 SD\") +\n  labs(x = \"\\nMean PEFR of two instruments (l/min)\",\n       y = \"Difference in PEFR (PEFR1-PEFR2)\")\n\n\n\n\n\n\n\n\nFigure 2: Bland-Altman plot of the difference between the measurements against the mean and sd.\n\n\n\n\n\n\n\n\nStep 3: Interpretation\nLet’s interpret Figure 2. There is considerable lack of agreement between PEFR1 and PEFR2 with discrepancies of up to 80 l/min. We see no obvious patterns in the plot. Because there is no pattern, we can summarize the lack of agreement by calculating the bias as the mean of the differences: -2.1 and a sd of 38.8.\n\n\nAn example of a pattern could be that the agreement gets better or worse at higher values of the x-axis. In this case, it would not make see to calculate the bias because that depends on the value of the mean. \n\n\nCode\nba %&gt;% \n  ggplot(aes(x = diff)) +\n  geom_histogram(aes(y = ..density..), binwidth = 2*IQR(ba$diff)/length(ba$diff)^(1/3), color = \"white\") +\n  geom_density() +\n  labs(y = \"\",\n       x = \"Difference (PEFR1-PEFR2)\")\n\n\n\n\n\n\n\n\n\nFigure 3: Histogram of differences.\n\n\n\n\nIf the differences follow a normal distribution (something that would be expected), then we can infer that 95% of the differences would lie between +/- 2 standard deviations of the mean (1.96 to be precise). In any event, one can plot a histogram to get a sense of the distribution, as shown in Figure 3.\nWe then calculate the limits of agreement as shown in the graph as follows:\n\\[bias-2sd=-2.1-(2*38.8)=-80\\] \\[bias+2sd=-2.1+(2*38.8)=76\\]\nNow, refer back to step 0: if differences as large as +/-2sd are deemed of no practical importance by the subject matter experts, then, the two measurement methods could be used interchangeably. However, if differences as large as +/- 2sd are unacceptable, then the two measurement do not agree and the new meter (PEFR2), having differences of 80 l/min below, or 76 l/min above the old meter (PEFR1), is not acceptable.\n\n\n\nNotes\n\nIf we see that the scatter in Figure 2 increases as the mean PEFR increases, we can try a log-transform, apply the analysis, and then back-transform the findings to the original scale."
  },
  {
    "objectID": "posts/2023-08-14-Assessing-the-Effect-of-an-Intervention/index.html",
    "href": "posts/2023-08-14-Assessing-the-Effect-of-an-Intervention/index.html",
    "title": "Assessing the Effect of an Intervention",
    "section": "",
    "text": "Background\nWe sometimes need to evaluate if an intervention is effective. For instance, in 2015, I was asked to assess whether a particular in-situ technology was associated with increased oil rate.\n\n\n{{&lt; fa square-arrow-up-right &gt;}} Frank Harrell chapter on complex curve fitting (2-54)\n {{&lt; fa square-arrow-up-right &gt;}} Journal article on interrupted time series \nThese analyses tend to be difficult not only because the data is noisy and some of the measurements are not accurate, but also because the presence of confounders that make the relationship unclear.\n\n\n A confounder is an extraneous variable whose presence affects the variables being studied so that the results do not reflect the actual relationship between the variables under study.\n\nThese type of analyses go by the name of quasi-experimental or quasi-causal.\nFor instance, suppose we have a process that looks like this:\n\nWe apply an intervention at a point in time and want to assess whether the intervention is associated with a change in the response. Perhaps the process has some cyclicality and it’s trending up. How can we assess whether such association exist without being fooled by other existing patterns?\nThe aim of this notebook is to demonstrate a particular method of analysis based on interrupted time-series.\n\n\n\nData\n\nResponse: Acute Cardiac Events (aces) from Sicily, Italy\nIntervention: smoking ban at all indoor locations in January 2005\nQuestion: what is the effects of a smoking ban on aces?\n\n\n\nCode\ngetHdata(sicily)\n\nsicily %&gt;%\n  slice(1:5) %&gt;%\n  select(time, year, everything()) %&gt;%\n  kable(digits = 0, caption = \"Sample 10 rows of data\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"responsive\", \"compact\"), full_width = F)\n\n\n\n\nSample 10 rows of data\n\n\ntime\nyear\nmonth\naces\nsmokban\npop\nstdpop\nrate\n\n\n\n\n1\n2002\n1\n728\n0\n364277.4\n379875.3\n191.6418\n\n\n2\n2002\n2\n659\n0\n364277.4\n376495.5\n175.0353\n\n\n3\n2002\n3\n791\n0\n364277.4\n377040.8\n209.7916\n\n\n4\n2002\n4\n734\n0\n364277.4\n377116.4\n194.6349\n\n\n5\n2002\n5\n757\n0\n364277.4\n377383.4\n200.5918\n\n\n\n\n\n\n\n\n\n\nCode\n## Make proper dates\nsicily &lt;- sicily %&gt;%\n  mutate(date = paste0(year, \"-\", month, \"-\", \"1\")) %&gt;%\n  mutate(date = ymd(date)) %&gt;%\n  mutate(month = as.factor(as.numeric(month)))\n\n## Graph it\n(sicily %&gt;%\n  ggplot(aes(x = date, y = aces, color = month)) +\n  geom_point(size = 2, show.legend = FALSE) +\n  labs(\n    y = \"Acute Cardiac Events (aces)\",\n    x = \"\",\n    color = \"\"\n  ) +\n  theme(legend.position = \"none\")) %&gt;%\n  ggplotly()\n\n\n\n\nAcute Cardiac Events (aces) over time. Color denotes months. Notice how ACES peak in the winter and drop in the summer.\n\n\n\n\n\nCode\n(sicily %&gt;%\n  ggplot() +\n  geom_point(aes(x = date, y = aces, color = month), size = 2, show.legend = FALSE) +\n  geom_smooth(aes(x = date, y = aces), span = 0.30, se = FALSE, color = \"grey30\", linetype = \"dotdash\", size = 0.6) +\n  labs(\n    y = \"Acute Cardiac Events (aces)\",\n    x = \"\",\n    color = \"\"\n  ) +\n  theme(legend.position = \"none\")) %&gt;%\n  ggplotly()\n\n\n\n\nAcute Cardiac Events (aces) over time with superimposed LOWESS smoother. Notice the cyclical trends becoming more apparent.\n\n\n\n\n\nCapturing the cyclicality of the data\nWe need to first capture the cyclicality of the data (otherwise, we cannot untangle a natural up or down trend from the effect of the intervention). These cyclical trends are not only apparent from the graph, but coronary events are known to follow cyclic variation (see: Douglas et al.).\nWe start with a restricted cubic splines with 6 knots. Since we are dealing with counts, it is common to employ a Poisson regression, a special case of Generalized Linear Model (GLM). Harrell also adjusts for population size as an offset variable .\n\n\nAn offset is “a component of a linear predictor that is known in advance (typically from theory, or from a mechanistic model of the process). Because it is known, it requires no parameter to be estimated from the data. For linear models with normal errors an offset is redundant. For Generalized Linear Models (GLM), however, it is necessary to specify part of the variation in the response using an offset. (Ghilagaber, 2008). An offset is used for a covariate with known slope. This might arise in situations where you are correcting the number of events for an estimate of population size (Thomas).\nThe AIC of this tentative model is:\n\n\nCode\ndd &lt;- datadist(sicily)\noptions(datadist = \"dd\")\n\ng &lt;- function(x) exp(x) * 100000\noff &lt;- list(stdpop = mean(sicily$stdpop))\n\nf0 &lt;- Glm(aces ~ offset(log(stdpop)) + rcs(time, 6), data = sicily, family = poisson)\nf0$aic\n\n\n[1] 721.5237\n\n\n\n\nCode\nggplot(Predict(f0, fun = g, offset = off)) +\n  geom_point(aes(x = time, y = rate), data = sicily) +\n  geom_vline(aes(xintercept = 37, col = \"red\")) +\n  annotate(\"text\", x = 42, y = 189, label = \"intervention\") +\n  labs(\n    y = \"Acute Coronary Cases Per 100,000\",\n    x = \"Months since start\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\nAcute Cardiac Events (aces) over time with a 6-knots restricted cubic spline on time from a Poisson model adjusted for population size.\n\n\n\n\n\nThis is a good start. Now we need to add seasonality to the model. This can be done by adding a sine/cosine terms.\n\n\n{{&lt; fa square-arrow-up-right &gt;}} Accounting for cyclical trends in models If we knew the origin, then we could just use one Sin term. In this case, it appears the lowest incidence occurs in the summer (August). However, in this example we will estimate the origin with both a sine and cosine term.\nWe can safely assume the period to be 12 months. Let’ save and print the 6 knots locations:\n\n\nCode\n## Save knots locations\nk &lt;- attr(rcs(sicily$time, 6), \"parms\")\nk\n\n\n[1]  5.00 14.34 24.78 35.22 45.66 55.00\n\n\nNow let’s add the sin/cos function to time and re-fir the model:\n\nkn &lt;- k\nh &lt;- function(x) cbind(rcspline.eval(x, kn), sin = sin(2 * pi * x / 12), cos = cos(2 * pi * x / 12))\n\nf1 &lt;- Glm(aces ~ offset(log(stdpop)) + gTrans(time, h), data = sicily, family = poisson)\nf1$aic\n\n[1] 674.112\n\n\n\n\nCode\nggplot(Predict(f1, fun = g, offset = off)) +\n  geom_point(aes(x = time, y = rate), data = sicily) +\n  geom_vline(aes(xintercept = 37, col = \"red\")) +\n  labs(\n    y = \"Acute Coronary Cases Per 100,000\",\n    x = \"Months since start\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\nAcute Cardiac Events (aces) over time with a sin/cos function of time from a Poisson model adjusted for population size.\n\n\n\n\nThis looks much better already, and the AIC confirms it (674 vs. 722 - lower is better).\n\n\n\nIntervention\nConventional interrupted time series assumes a discontinuity at the intervention point. Since the intervention occurs at month 37 (i.e. January 2005), we start by adding 3 knots at 36, 37 and 38 months. Notice that the sin/cos function has been updated with the additional knots for a toal of 9 knots.\n\nkn &lt;- sort(c(k, c(36, 37, 38)))\nf2 &lt;- Glm(aces ~ offset(log(stdpop)) + gTrans(time, h), data = sicily, family = poisson)\nf2$aic\n\n[1] 661.7904\n\n\n\n\nCode\nggplot(Predict(f2, fun = g, offset = off)) +\n  geom_point(aes(x = time, y = rate), data = sicily) +\n  geom_vline(aes(xintercept = 37, col = \"red\")) +\n  labs(\n    y = \"Acute Coronary Cases Per 100,000\",\n    x = \"Months since start\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\nAcute Cardiac Events (aces) over time with a sin/cos function of time and additional knots around the interventionto allow for a sudden change from a Poisson model adjusted for population size.\n\n\n\n\n\nNine knots is a lot given the sample size of 59. We now simplify the number of knots (6 instead of 9) and add a sudden discontinuity at 37 months:\n\nh &lt;- function(x) {\n  cbind(rcspline.eval(x, k),\n    sin = sin(2 * pi * x / 12), cos = cos(2 * pi * x / 12),\n    jump = x &gt;= 37\n  )\n}\n\nf3 &lt;- Glm(aces ~ offset(log(stdpop)) + gTrans(time, h), data = sicily, family = poisson)\nf3$aic\n\n[1] 659.6044\n\n\n\n\nCode\ntimes &lt;- sort(c(seq(0, 60, length = 200), 36.999, 37, 37.001))\n\nggplot(Predict(f3, time = times, fun = g, offset = off)) +\n  geom_point(aes(x = time, y = rate), data = sicily) +\n  geom_vline(aes(xintercept = 37, col = \"red\")) +\n  labs(\n    y = \"Acute Coronary Cases Per 100,000\",\n    x = \"Months since start\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\nAcute Cardiac Events (aces) over time with a sin/cos function of time and a discontinuity at the intervention from a Poisson model adjusted for population size.\n\n\n\n\n\n\n\nAssessing the Effect\nWe now focus on quantifying the evidence on the effect of the intervention.\n\n\nCode\noptions(prType = \"html\")\nf3\n\n\n\nGeneral Linear Model\n\nGlm(formula = aces ~ offset(log(stdpop)) + gTrans(time, h), family = poisson, \n    data = sicily)\n\n\n\n\n\n\n\n\n\n\n\nModel Likelihood\nRatio Test\n\n\n\n\nObs 59\nLR χ2 169.64\n\n\nResidual d.f. 51\nd.f. 7\n\n\ng 0.080\nPr(&gt;χ2) &lt;0.0001\n\n\n\n\n\n\n\n\n\n\n\nβ\nS.E.\nWald Z\nPr(&gt;|Z|)\n\n\n\n\nIntercept\n -6.2118\n 0.0095\n-656.01\n&lt;0.0001\n\n\ntime\n  0.0635\n 0.0113\n5.63\n&lt;0.0001\n\n\ntime'\n -0.1912\n 0.0433\n-4.41\n&lt;0.0001\n\n\ntime''\n  0.2653\n 0.0760\n3.49\n0.0005\n\n\ntime'''\n -0.2409\n 0.0925\n-2.61\n0.0092\n\n\nsin\n  0.0343\n 0.0067\n5.11\n&lt;0.0001\n\n\ncos\n  0.0380\n 0.0065\n5.86\n&lt;0.0001\n\n\njump\n -0.1268\n 0.0313\n-4.06\n&lt;0.0001\n\n\n\n\n\n\nFrom the table of coefficients we see strong evidence in support of the intervention. In particular, the coefficient for the jump is \\(-0.127\\). Because Poisson regression models the log of the expected count, the coefficient reflects the difference in log-counts of acute cardiac event associated with the intervention. Since the difference is negative, it means that the intervention is protective (i.e. it appears to be working). We can make the coefficient more interpretable by exponentiating it and say that the intervention is associated with a \\(\\left [1-exp^{-0.127}  \\right ]*100 = 11.9\\%\\) reduction in acute cardiac event.\n\n\n\n\n\n\n\nConclusion\n\n\n\nAccording to the model, the intervention is associated with a \\(12\\%\\) reduction in acute cardiac event, and between \\(6\\%\\) and \\(17\\%\\), 95% of the times.\n\n\n\n\n\nImproving the estimate\nCalculations of robust standard errors to be used in the 95% confidence interval above.\n\n\nCode\n## Source: https://stats.oarc.ucla.edu/r/dae/poisson-regression/\nlibrary(sandwich)\nf3.1 &lt;- glm(aces ~ offset(log(stdpop)) + gTrans(time, h), data = sicily, family = poisson)\n\ncov.m1 &lt;- vcovHC(f3.1, type = \"HC0\")\nstd.err &lt;- sqrt(diag(cov.m1))\nr.est &lt;- cbind(Estimate = coef(f3.1), \"Robust SE\" = std.err, \"Pr(&gt;|z|)\" = 2 * pnorm(abs(coef(f3.1) / std.err), lower.tail = FALSE), LL = coef(f3.1) - 1.96 * std.err, UL = coef(f3.1) + 1.96 * std.err)\nexp(r.est[8, -3])\n\n\n Estimate Robust SE        LL        UL \n0.8808974 1.0337250 0.8254514 0.9400677 \n\n\n\n\n\nReal World Example\nEarlier, I alluded to the application of this method to the analysis of a technology intervention for in-situ well. Indeed, this post was written when I wanted re-analyze the data using this modified technique. Naturally, I cannot share neither the example nor the findings."
  },
  {
    "objectID": "posts/2023-08-28-prediction-vs-inference/index.html#conclusion",
    "href": "posts/2023-08-28-prediction-vs-inference/index.html#conclusion",
    "title": "Prediction vs. Inference Cultures: Same Toolbox, Different Focus",
    "section": "Conclusion",
    "text": "Conclusion\nIt certainly feels like we have moved from one extreme to the other: for more than a century we have focused on the data culture, that is, inference, unbiased estimates, causation and data generating processes. Breiman hoped the field would pay more attention to algorithmic (i.e. pure prediction) approaches. Little would he know (he died in 2005), that the world seems to be overwhelmingly focused on prediction and prediction only today. This is not to say prediction is not important. Far from it. Rather, that the vast majority of practitioners are blind to inference."
  },
  {
    "objectID": "posts/2023-08-28-prediction-vs-inference/index.html#resources",
    "href": "posts/2023-08-28-prediction-vs-inference/index.html#resources",
    "title": "Prediction vs. Inference Cultures: Same Toolbox, Different Focus",
    "section": "Resources",
    "text": "Resources\n\nhttps://arxiv.org/abs/1811.10154\n\nhttps://med.stanford.edu/content/dam/sm/dbds/documents/biostats-workshop/paper-1-.pdf\nhttps://www.fharrell.com/post/split-val/\n\nBerk, Richard A. Statistical Learning from a Regression Perspective. Springer Texts in Statistics. Cham: Springer International Publishing, 2020. https://doi.org/10.1007/978-3-030-40189-4."
  },
  {
    "objectID": "posts/2024-11-28-changes-in-ds/index.html",
    "href": "posts/2024-11-28-changes-in-ds/index.html",
    "title": "A Changing Environment in Data Science",
    "section": "",
    "text": "The field of Data Science has gone through subtle but important changes in the past 8 years or so. It helps to pause and look back:\n\nDS originated in the scientific community. Researchers used DS tools and mindset to test hypotheses, collect data, predict outcomes.\nAs computing, storage, bandwidth cost shrunk, these disciplines entered business and gov’t\nAs DS entered business, it came with its own set of non-proprietary tools and a more open and collaborative mindset. At times, this caused frictions in companies that for decades had relied on proprietary technologies and locked-down computing environments\nLarge technology firms, saw the writing on the wall and slowly ushered their clients into a new era, where data, computing and collaboration became central features\nBut that word, “science”, was always problematic for tech firms and organizations alike: science relies on experimentation to explore and understand the world, often leading to failures that either yield no new knowledge or challenge our existing hypotheses. Science is uncertainty.\nIn the business world, where uncertainty is often eschewed, the scientific approach of relying on experimentation and embracing failures can be challenging. In 2018, MS Azure included features for running and managing machine learning “experiments”.\nIn the past few years, we have seen companies moving away from the uncertain world of the scientific approach. There are clear signs of this. First, DS teams have largely been brought under IT, where the prevailing mindset is completing tasks on time and on budget. This is at odd with science. People coming from traditional software development and IT tend to struggle empathizing with this idea. Katie Malone put it eloquently: “[in IT/software development] there’s the notion that if you keep working on something, you’re going to get there. People who are used to thinking this way can sometimes get pretty confused when a data scientist comes back and says: I’m sorry, this is isn’t working and there’s nothing we can do about that. That sounds like a foreign idea”. Second, supply/demand of professionals in DS has largely shifted from pure STEM (examples: both NYT WAPO head of viz in 2016~2018 came as STEM researchers), towards IT disciplines which tend to lack training and mindset for scientific thinking (rigorous experimentation, embracing uncertainty, and iterative learning).\nAs a result, the DS field has largely shifted away from the science part and more towards the data side.\n“Insight generation”, a tenet of the scientific process and, in particular, of Exploratory Data Analysis (EDA), became secondary. “Data Products” became a far more important idea. Automating data pipelines, creating sophisticated metrics, deploying a predictive tool in production, deploying an LLM are some of the common tasks in the DS space these days. Most of all, seldom do data products come with uncertainty or require extensive experimentation. In fact, data products can be completed on time and on budget."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Thomas Speidel",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nExploring Changes in Temperatures in Calgary\n\n\n\n\n\n\neda\n\n\n\nIn this notebook I explore 70 years of weather data in Calgary, Canada.\n\n\n\n\n\nJul 28, 2024\n\n\n123 min\n\n\n\n\n\n\n\nAgreement Between Instruments\n\n\n\n\n\n\nmethods\n\n\n\nAssessing and comparing agreement among measurement instruments.\n\n\n\n\n\nAug 31, 2023\n\n\n16 min\n\n\n\n\n\n\n\nA Changing Environment in Data Science\n\n\n\n\n\n\nopinion-editorial\n\n\n\n.\n\n\n\n\n\nAug 31, 2023\n\n\n6 min\n\n\n\n\n\n\n\nPrediction vs. Inference Cultures: Same Toolbox, Different Focus\n\n\n\n\n\nThe role of accuracy metrics in prediction vs. inference.\n\n\n\n\n\nAug 28, 2023\n\n\n8 min\n\n\n\n\n\n\n\nAssessing the Effect of an Intervention\n\n\n\n\n\n\nmethods\n\n\n\nThis is a practice notebook illustrating methods of analyzing whether an intervention is effective.\n\n\n\n\n\nAug 14, 2023\n\n\n12 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "colophon.html",
    "href": "colophon.html",
    "title": "Colophon",
    "section": "",
    "text": "The blog was created in R using the Quarto publishing system. This is a suitable modern technical authorship system.\nThe look and feel of the site is heavily inspired by Edward Tufte. I wanted to convey an elegant yet rich experience for the reader wuthout sacrificing readability. Tufte spent many decades perfecting his publishing style and it became the natural inspiration for this project.\nTufte designed most of his work for print. The people behind Quarto already incorporated Tufte’s elements in their framework. I remember Tint which used rmarkdown several years ago. With the move from rmarkdown to Quarto, incorporating Tufte-esque elements became a lot more straight forward. For instance, Tufte’s famous sidenotes, now only require {.column-margin}.\nIn the end, I had to work on two areas to better adapt Tufte’s style to the web: typography and colors. I’m not good with css, I mostly write css via trial and error."
  },
  {
    "objectID": "colophon.html#fonts",
    "href": "colophon.html#fonts",
    "title": "Colophon",
    "section": "Fonts",
    "text": "Fonts\nTufte uses Gill Sans and ET Bembo, neither of which exist as webfonts in Google. ET Bembo is the serif type font used for the main body, similar to Garamond. While there are no identical web versions EB Garamond and Cardo are very similar and both available in Googlefonts. In the end I opted for Cardo.\nGill Sans is the other font used Tufte. This is a non-serif font used in tables and certain headings. Again, no webversion exists of Gill Sans in Googlefonts. I opted for Lato which is somewhat similar to Gill Sans.\n I decided to leave the main heading such as the titles as Lato.\nFor code blocks, I used space mono, a font I have long used as my monospaced code editing choice."
  },
  {
    "objectID": "colophon.html#colors",
    "href": "colophon.html#colors",
    "title": "Colophon",
    "section": "Colors",
    "text": "Colors\nI kept colors to a minimum. The body color is nearly black (#212529), the primary bootstrap color is red. The background page color is a off white (#FFFFF8)."
  },
  {
    "objectID": "colophon.html#graphs",
    "href": "colophon.html#graphs",
    "title": "Colophon",
    "section": "Graphs",
    "text": "Graphs\nGraphs follow a similar style.\n\n\nCode\ntheme_set(theme_minimal(base_family = \"Pragmata Pro Mono\") +\n            theme(\n              plot.title = element_text(family = \"Lato\", face = \"bold\"),\n              plot.subtitle = element_text(family = \"Lato\"),\n              axis.text = element_text(family = \"Pragmata Pro Mono\"),\n              axis.title = element_text(family = \"Lato\", hjust = 1),\n              panel.spacing = unit(2, \"lines\"),\n              strip.text.x = element_text(family = \"Lato\"),\n              strip.background =element_rect(fill = \"Grey90\"),\n              panel.grid.minor = element_blank(),\n              panel.grid.major = element_line(size = 0.2),\n              legend.text = element_text(family = \"Lato\"),\n              legend.title = element_text(family = \"Lato\", face = \"bold\"),\n              panel.background = element_rect(fill = '#FFFFF8', color = '#FFFFF8'),\n              plot.background = element_rect(fill = \"#FFFFF8\", color = '#FFFFF8')\n            ))"
  }
]