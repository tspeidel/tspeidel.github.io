[
  {
    "objectID": "posts/2023-08-31-agreement-btw-instruments/index.html",
    "href": "posts/2023-08-31-agreement-btw-instruments/index.html",
    "title": "Agreement Between Instruments",
    "section": "",
    "text": "We sometimes need to evaluate whether a new instrument agrees with an existing one. Instrument is loosely meant as a measuring device such as a sensor or an online analyzer, but it could also be a person measuring or rating something. Therefore, we wish to quantify how much the two agree/disagree."
  },
  {
    "objectID": "posts/2023-08-31-agreement-btw-instruments/index.html#correlation-coefficients",
    "href": "posts/2023-08-31-agreement-btw-instruments/index.html#correlation-coefficients",
    "title": "Agreement Between Instruments",
    "section": "Correlation Coefficients",
    "text": "Correlation Coefficients\nThere exist correlation coefficients that have been developed for these tasks. However, a critique of using correlation coefficients is that: “a perfect correlation can result even when the measurements disagree by a factor of 10” (Harrell, 1987).\nThe intraclass correlation coefficient (ICC) is a common one used for these tasks. Others include Cohen’s kappa statistic, Concordance Correlation Coefficient.\n\n\nSee: https://hbiostat.org/bbr/obsvar"
  },
  {
    "objectID": "posts/2023-08-31-agreement-btw-instruments/index.html#bland-altman-plot",
    "href": "posts/2023-08-31-agreement-btw-instruments/index.html#bland-altman-plot",
    "title": "Agreement Between Instruments",
    "section": "Bland-Altman Plot",
    "text": "Bland-Altman Plot\nA sensible visualization of the agreement between two measurements has been proposed by Bland and Altman. This happens to be the same as Tukey mean-difference plot. Bland and Altman develop this method to address the same shortcoming already pointed out earlier, namely, that “a high correlation does not necessarily imply that there is good agreement between the two methods”.\n\n\nYou can read the original paper here. Bland and Altman wrote in the Clinical Chemistry Journal (63:10 (2017)): “We certainly were not the first people to recognize that correlation coefficients did not assess agreement but rather association. Notably, we quoted Westgard and Hunt, who wrote wisely in Clinical Chemistry in 1973, “The correlation coefficient is of no practical use in the statistical analysis of comparison data”.”\n\n\n\n\nA peak expiratory flow rate (PEFR). Credit: healthline\n\n\nHere we illustrate the method using the authors original data on peak expiratory flow rate (PEFR) measures via two instruments: pefr1 and pefr2 on 17 subjects.\n\n## Import data used by Bland and ALtman in original paper\nba &lt;- read_csv(\"Data/bland-altman.csv\")\n\nba %&gt;%\n  kable(escape = T, format = \"html\", digits = 1, align = rep(\"c\", 5)) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"responsive\", \"compact\"), full_width = F)\n\n\n\n\nTable 1: Original data on PEFR1 and PEFR2 measruments.\n\n\nsubject\npefr1\npefr2\n\n\n\n\n1\n494\n512\n\n\n2\n395\n430\n\n\n3\n516\n520\n\n\n4\n434\n428\n\n\n5\n476\n500\n\n\n6\n557\n600\n\n\n7\n413\n364\n\n\n8\n442\n380\n\n\n9\n650\n658\n\n\n10\n433\n445\n\n\n11\n417\n432\n\n\n12\n656\n626\n\n\n13\n267\n260\n\n\n14\n478\n477\n\n\n15\n178\n259\n\n\n16\n423\n350\n\n\n17\n427\n451\n\n\n\n\n\n\n\n\n\nFor completeness, we show the estimated conventional correlation coefficient (i.e. Pearson) and the scatterplot, knowing that this (i.e. correlation) is not an appropriate way to analyze these data:\n\ncor(ba$pefr1, ba$pefr2, method = \"pearson\")\n\n[1] 0.9432794\n\n\nAs the authors write in the paper: “the high correlation of 0.94 for our own data conceals considerable lack of agreement between the two instruments”.\nWe want to know by how much the new method is likely to differ from the old. Ideally, we want to have a sense of the value beyond which we would say the two instruments are not in agreement. This is more a business decision than a data one, and needs to consider whether the difference between the two measurements is large enough to lead to a different decision.\n\n\nStep 0\nThere is a zero step in which we need to ask the subject matter expert the following question: how different does the second measurement need to be from the first measurement in order for it to lead to a different decision with regard to its use? Do this without peeking at the data, as it would lead to a cognitive bias (anchoring).\n\n\n\nStep 1\n\nba %&gt;%\n  ggplot(aes(x = pefr2, y = pefr1)) +\n  geom_point(shape = 1) +\n  geom_abline(intercept = 0, slope = 1, size = 0.25)\n\n\n\n\n\nFigure 1: PEFR1 vs PEFR2. The first step is to examine one measurement vs. the other with a 1:1 line as a reference\n\n\n\nWe first plot the two methods against each other as shown in Figure 1. This is a good start, however, the points will be clustered around the identity line thus making it difficult to assess between-method differences.\n\n\n\nStep 2\nWe next plot the differences between the measurements (on the y-axis), against the mean of the individual differences. We then add reference lines for the grand-mean of the differences, and +/- 2 standard deviation of the grand-mean. This is shown in Table 2 and Figure 2\n\n## Prepare diff, mean diff, sd of diff\nba &lt;- ba %&gt;%\n  mutate(diff = pefr1 - pefr2) %&gt;%\n  mutate(mean = rowMeans(select(., starts_with(\"pefr\")), na.rm = TRUE)) %&gt;%\n  mutate(diff_mean = mean(diff)) %&gt;%\n  mutate(diff_sd = sd(diff, na.rm = TRUE))\n\nba %&gt;%\n  slice(1:3) %&gt;%\n  kable(escape = T, format = \"html\", digits = 1, align = rep(\"c\", 5)) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"responsive\", \"compact\"), full_width = F)\n\n\n\n\nTable 2: Table of differences\n\n\nsubject\npefr1\npefr2\ndiff\nmean\ndiff_mean\ndiff_sd\n\n\n\n\n1\n494\n512\n-18\n503.0\n-2.1\n38.8\n\n\n2\n395\n430\n-35\n412.5\n-2.1\n38.8\n\n\n3\n516\n520\n-4\n518.0\n-2.1\n38.8\n\n\n\n\n\n\n\n\n\n\nba %&gt;%\n  ggplot(aes(x = mean, y = diff)) +\n  geom_point(shape = 1, size = 2) +\n  scale_y_continuous(breaks = seq(-90, 90, by = 20)) +\n  geom_hline(yintercept = ba$diff_mean, size = 0.5, linetype = \"dotted\") +\n  geom_hline(yintercept = ba$diff_mean + ba$diff_sd * 2, size = 0.25) +\n  geom_hline(yintercept = ba$diff_mean - ba$diff_sd * 2, size = 0.25) +\n  geom_hline(yintercept = 0, size = 0.25) +\n  annotate(\"text\", y = ba$diff_mean - 3.5, x = 620, family = \"Pragmata Pro Mono\", size = 3.6, label = \"Mean of differences\") +\n  annotate(\"text\", y = ba$diff_mean + ba$diff_sd * 2 + 4, x = 620, face = \"italic\", size = 3.6, family = \"Pragmata Pro Mono\", label = \"Mean + 2 SD\") +\n  annotate(\"text\", y = ba$diff_mean - ba$diff_sd * 2 - 4, x = 620, face = \"italic\", size = 3.6, family = \"Pragmata Pro Mono\", label = \"Mean - 2 SD\") +\n  labs(\n    x = \"\\nMean PEFR of two instruments (l/min)\",\n    y = \"Difference in PEFR (PEFR1-PEFR2)\"\n  )\n\n\n\n\nFigure 2: Bland-Altman plot of the difference between the measurements against the mean and sd.\n\n\n\n\n\n\n\nStep 3: Interpretation\nLet’s interpret Figure 2. There is considerable lack of agreement between PEFR1 and PEFR2 with discrepancies of up to 80 l/min. We see no obvious patterns in the plot. Because there is no pattern, we can summarize the lack of agreement by calculating the bias as the mean of the differences: -2.1 and a sd of 38.8.\n\n\nAn example of a pattern could be that the agreement gets better or worse at higher values of the x-axis. In this case, it would not make see to calculate the bias because that depends on the value of the mean. \n\nba %&gt;%\n  ggplot(aes(x = diff)) +\n  geom_histogram(aes(y = ..density..), binwidth = 2 * IQR(ba$diff) / length(ba$diff)^(1 / 3), color = \"white\") +\n  geom_density() +\n  labs(\n    y = \"\",\n    x = \"Difference (PEFR1-PEFR2)\"\n  )\n\n\n\n\n\nFigure 3: Histogram of differences.\n\n\n\nIf the differences follow a normal distribution (something that would be expected), then we can infer that 95% of the differences would lie between +/- 2 standard deviations of the mean (1.96 to be precise). In any event, one can plot a histogram to get a sense of the distribution, as shown in Figure 3.\nWe then calculate the limits of agreement as shown in the graph as follows:\n\\[bias-2sd=-2.1-(2*38.8)=-80\\] \\[bias+2sd=-2.1+(2*38.8)=76\\]\nNow, refer back to step 0: if differences as large as +/-2sd are deemed of no practical importance by the subject matter experts, then, the two measurement methods could be used interchangeably. However, if differences as large as +/- 2sd are unacceptable, then the two measurement do not agree and the new meter (PEFR2), having differences of 80 l/min below, or 76 l/min above the old meter (PEFR1), is not acceptable.\n\n\n\nNotes\n\nIf we see that the scatter in Figure 2 increases as the mean PEFR increases, we can try a log-transform, apply the analysis, and then back-transform the findings to the original scale."
  },
  {
    "objectID": "posts/2023-08-28-prediction-vs-inference/index.html",
    "href": "posts/2023-08-28-prediction-vs-inference/index.html",
    "title": "Prediction vs. Inference Cultures: Same Toolbox, Different Focus",
    "section": "",
    "text": "Prediction\nThe popularity of AI, ML and Data Science certainly owes a lot to advances in data intensive models that occurred in the last decade or two (such as backpropagation in neural networks). The main focus of these applications has been that of maximizing predictive power. Its success can be easily seen in applications of modestly high signal-to-noise ratio (e.g. image recognition, voice recognition, language models and so on).\nWhat do I mean by high signal-to-noise ratio? It’s a loose way to assess how much signal a particular dataset contains. In a predictive model this could translate to a high R2 . A simple heuristic I use (inspired by Andrew Ng), is to ask the following question: how quickly and easily would a human solve the same task? If the answer is a fraction of a second, as in recognizing a cat from a dog, the problem has a high signal-to-noise ratio and it’s probably a prime application for machine learning.\nThese type of applications are abundant these days. Indeed, most Data Science and Machine Learning curricula give an overwhelming emphases to prediction. To paraphrase Breiman, this is the prediction culture, one where the focus is squarely focused on minimizing out of sample prediction error.\n\n\nThe title is inspired by Leo Breiman famous paper on the two cultures, which is very relevant to this topic.\n\nTechnically, prediction is a type of inference. So, the better terminology should be predictive inference vs. causal inference.\n\n\n\nInference\nIt’s easy to forget the other use of modelling, one that makes the identification and understanding of causal factors its ultimate goal (I use “causal” quite loosely here). Again, to use Breimann’s terminology, this is the inference culture. Inference is nearly absent in today’s Data Science and ML curricula. I want to quote from a CrossValidated thread:\n\n\nA more thorough explanation of this difference is provided by Berk.\n\nInference. You want to find out what the effect of Age, Passenger Class and, Gender has on surviving the Titanic Disaster. You can put up a logistic regression and infer the effect each passenger characteristic has on survival rates.\n\n\nPrediction: Given some information on a Titanic passenger, you want to choose from the set {lives,dies} and be correct as often as possible. Prediction doesn’t revolve around establishing the most accurate relation between the input and the output, accurate prediction cares about putting new observations into the right class as often as possible.\n\nIn my experience, when the purely predictive applications don’t pan out, we simply throw away the baby with the bathtub water. Why? I suspect in many cases the reason is simply that practitioners don’t know about inference. And even when they do know about it, misconceptions around it abound. And there are a lot of misconceptions, here are a few tide to regression.\n\n\n\nAccuracy Metrics\nRecall that the inferential culture is focused on identifying and understanding causal factors. We do not need a highly predictive model to achieve that. Instead, we want to identify “which input has such a relationship, and how can we distinguish a ”true” relationship from random covariation”.\nA low R2 (or RMSE or any similar metric) simply means that the model as-is did not explain much of the variation in the response. For purely predictive purposes, a low R2 may be enough to kill an entire project.\n\n\nIt’s instructive to read the article by Cummings et al. on reporting statistical information in medical journal articles:\n\nestimates of variance explained such as R2, correlation coefficients, and standardized regression coefficients are not useful measures of causal associations or agreement and should not be presented as the main results of an analysis.\n\nHowever, when the is interest inference, insight can still be gained with low R2. In fact, low R2 are the norm in medical research, social science and economics to name a few. In these fields, practitioners do not expect to account for all relevant features (it is simply not possible). Instead, the focus is on the individual features and whether or not their presence in the model is of statistical (and practical) significance.\nTherefore, the focus in these tasks is on certain aspects of the feature in the model such as standard errors, p-values, effect size, the potential for interaction and so on: if a feature is statistically significant and its effect of some practical significance, that becomes valuable insight, irrespective of a not so high R2.\n\n\n\nSuggestions for Practitioners\nMost data science practitioners come from a pure prediction culture: it may sound disconcerting to learn that some of the routines they are used to are not used or, sometimes, counter-productive when the aim is inference.\n\n\nThese differences come up more and more as students are often exposed to the two cultures when they take statistics classes and ML ones at the same time. For instance in this reddit thread\nHere are some suggestions:\n\nTry to assess as early as possible whether the ultimate goal is pure prediction or inference\nIf your goal is pure prediction and you care a lot about feature importance, consider switching to an inferential approach (see Efron)\nFor many applications where the signal to noise ratio is low, and the pure prediction approach yields too low accuracy, consider using inference instead of cancelling the whole project\nIf you use inference you need to level set (i.e. lower) the client’s expectations, since they are probably calibrated around high predictive accuracy\nIf you are using inference you need to think differently about the “product”. A model deployed in production here likely makes little sense (and will inflate the customer’s expectations). You may want to focus on how to best communicate the insight or incorporate the insight in a dashboard. For example, features that are both staticially and practically significant could be incorporated in monitoring dashboards. Perhaps, run charts could be used to monitor for significant changes in the feature."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Thomas Speidel",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nAgreement Between Instruments\n\n\n\n\n\n\n\nmethods\n\n\n\n\nAssessing and comparing agreement among measurement instruments.\n\n\n\n\n\n\nAug 31, 2023\n\n\n10 min\n\n\n\n\n\n\n\n\nPrediction vs. Inference Cultures: Same Toolbox, Different Focus\n\n\n\n\n\nThe role of accuracy metrics in prediction vs. inference.\n\n\n\n\n\n\nAug 28, 2023\n\n\n8 min\n\n\n\n\n\n\n\n\nAssessing the Effect of an Intervention\n\n\n\n\n\n\n\nmethods\n\n\n\n\nThis is a practice notebook illustrating methods of analyzing whether an intervention is effective.\n\n\n\n\n\n\nAug 14, 2023\n\n\n12 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "colophon.html",
    "href": "colophon.html",
    "title": "Colophon",
    "section": "",
    "text": "The blog was created in R using the Quarto publishing system. This is a suitable modern technical authorship system.\nThe look and feel of the site is heavily inspired by Edward Tufte. I wanted to convey an elegant yet rich experience for the reader wuthout sacrificing readability. Tufte spent many decades perfecting his publishing style and it became the natural inspiration for this project.\nTufte designed most of his work for print. The people behind Quarto already incorporated Tufte’s elements in their framework. I remember Tint which used rmarkdown several years ago. With the move from rmarkdown to Quarto, incorporating Tufte-esque elements became a lot more straight forward. For instance, Tufte’s famous sidenotes, now only require {.column-margin}.\nIn the end, I had to work on two areas to better adapt Tufte’s style to the web: typography and colors. I’m not good with css, I mostly write css via trial and error."
  },
  {
    "objectID": "colophon.html#fonts",
    "href": "colophon.html#fonts",
    "title": "Colophon",
    "section": "Fonts",
    "text": "Fonts\nTufte uses Gill Sans and ET Bembo, neither of which exist as webfonts in Google. ET Bembo is the serif type font used for the main body, similar to Garamond. While there are no identical web versions EB Garamond and Cardo are very similar and both available in Googlefonts. In the end I opted for Cardo.\nGill Sans is the other font used Tufte. This is a non-serif font used in tables and certain headings. Again, no webversion exists of Gill Sans in Googlefonts. I opted for Lato which is somewhat similar to Gill Sans.\n I decided to leave the main heading such as the titles as Lato.\nFor code blocks, I used space mono, a font I have long used as my monospaced code editing choice."
  },
  {
    "objectID": "colophon.html#colors",
    "href": "colophon.html#colors",
    "title": "Colophon",
    "section": "Colors",
    "text": "Colors\nI kept colors to a minimum. The body color is nearly black (#212529), the primary bootstrap color is red. The background page color is a off white (#FFFFF8)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Agreement Between Instruments\n\n\n\n\n\n\n\n\n\nAug 31, 2023\n\n\n\n\n\n\n\n\nPrediction vs. Inference Cultures: Same Toolbox, Different Focus\n\n\n\n\n\n\n\n\n\nAug 28, 2023\n\n\n\n\n\n\n\n\nAssessing the Effect of an Intervention\n\n\n\n\n\n\n\n\n\nAug 14, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-08-14-Assessing-the-Effect-of-an-Intervention/index.html",
    "href": "posts/2023-08-14-Assessing-the-Effect-of-an-Intervention/index.html",
    "title": "Assessing the Effect of an Intervention",
    "section": "",
    "text": "Background\nWe sometimes need to evaluate if an intervention is effective. For instance, in 2015, I was asked to assess whether a particular in-situ technology was associated with increased oil rate.\n\n\n Frank Harrell chapter on complex curve fitting (2-54)\n  Journal article on interrupted time series \nThese analyses tend to be difficult not only because the data is noisy and some of the measurements are not accurate, but also because the presence of confounders that make the relationship unclear.\n\n\n A confounder is an extraneous variable whose presence affects the variables being studied so that the results do not reflect the actual relationship between the variables under study.\n\nThese type of analyses go by the name of quasi-experimental or quasi-causal.\nFor instance, suppose we have a process that looks like this:\n\nWe apply an intervention at a point in time and want to assess whether the intervention is associated with a change in the response. Perhaps the process has some cyclicality and it’s trending up. How can we assess whether such association exist without being fooled by other existing patterns?\nThe aim of this notebook is to demonstrate a particular method of analysis based on interrupted time-series.\n\n\n\nData\n\nResponse: Acute Cardiac Events (aces) from Sicily, Italy\nIntervention: smoking ban at all indoor locations in January 2005\nQuestion: what is the effects of a smoking ban on aces?\n\n\ngetHdata(sicily)\n\nsicily %&gt;%\n  slice(1:5) %&gt;%\n  select(time, year, everything()) %&gt;%\n  kable(digits = 0, caption = \"Sample 10 rows of data\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"responsive\", \"compact\"), full_width = F)\n\n\n\nSample 10 rows of data\n\n\ntime\nyear\nmonth\naces\nsmokban\npop\nstdpop\nrate\n\n\n\n\n1\n2002\n1\n728\n0\n364277.4\n379875.3\n191.6418\n\n\n2\n2002\n2\n659\n0\n364277.4\n376495.5\n175.0353\n\n\n3\n2002\n3\n791\n0\n364277.4\n377040.8\n209.7916\n\n\n4\n2002\n4\n734\n0\n364277.4\n377116.4\n194.6349\n\n\n5\n2002\n5\n757\n0\n364277.4\n377383.4\n200.5918\n\n\n\n\n\n\n\n\n\n## Make proper dates\nsicily &lt;- sicily %&gt;%\n  mutate(date = paste0(year, \"-\", month, \"-\", \"1\")) %&gt;%\n  mutate(date = ymd(date)) %&gt;%\n  mutate(month = as.factor(as.numeric(month)))\n\n## Graph it\n(sicily %&gt;%\n  ggplot(aes(x = date, y = aces, color = month)) +\n  geom_point(size = 2, show.legend = FALSE) +\n  labs(\n    y = \"Acute Cardiac Events (aces)\",\n    x = \"\",\n    color = \"\"\n  ) +\n  theme(legend.position = \"none\")) %&gt;%\n  ggplotly()\n\n\n\nAcute Cardiac Events (aces) over time. Color denotes months. Notice how ACES peak in the winter and drop in the summer.\n\n\n\n\n(sicily %&gt;%\n  ggplot() +\n  geom_point(aes(x = date, y = aces, color = month), size = 2, show.legend = FALSE) +\n  geom_smooth(aes(x = date, y = aces), span = 0.30, se = FALSE, color = \"grey30\", linetype = \"dotdash\", size = 0.6) +\n  labs(\n    y = \"Acute Cardiac Events (aces)\",\n    x = \"\",\n    color = \"\"\n  ) +\n  theme(legend.position = \"none\")) %&gt;%\n  ggplotly()\n\n\n\nAcute Cardiac Events (aces) over time with superimposed LOWESS smoother. Notice the cyclical trends becoming more apparent.\n\n\n\n\n\nCapturing the cyclicality of the data\nWe need to first capture the cyclicality of the data (otherwise, we cannot untangle a natural up or down trend from the effect of the intervention). These cyclical trends are not only apparent from the graph, but coronary events are known to follow cyclic variation (see: Douglas et al.).\nWe start with a restricted cubic splines with 6 knots. Since we are dealing with counts, it is common to employ a Poisson regression, a special case of Generalized Linear Model (GLM). Harrell also adjusts for population size as an offset variable .\n\n\nAn offset is “a component of a linear predictor that is known in advance (typically from theory, or from a mechanistic model of the process). Because it is known, it requires no parameter to be estimated from the data. For linear models with normal errors an offset is redundant. For Generalized Linear Models (GLM), however, it is necessary to specify part of the variation in the response using an offset. (Ghilagaber, 2008). An offset is used for a covariate with known slope. This might arise in situations where you are correcting the number of events for an estimate of population size (Thomas).\nThe AIC of this tentative model is:\n\ndd &lt;- datadist(sicily)\noptions(datadist = \"dd\")\n\ng &lt;- function(x) exp(x) * 100000\noff &lt;- list(stdpop = mean(sicily$stdpop))\n\nf0 &lt;- Glm(aces ~ offset(log(stdpop)) + rcs(time, 6), data = sicily, family = poisson)\nf0$aic\n\n[1] 721.5237\n\n\n\nggplot(Predict(f0, fun = g, offset = off)) +\n  geom_point(aes(x = time, y = rate), data = sicily) +\n  geom_vline(aes(xintercept = 37, col = \"red\")) +\n  annotate(\"text\", x = 42, y = 189, label = \"intervention\") +\n  labs(\n    y = \"Acute Coronary Cases Per 100,000\",\n    x = \"Months since start\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\nAcute Cardiac Events (aces) over time with a 6-knots restricted cubic spline on time from a Poisson model adjusted for population size.\n\n\n\n\n\nThis is a good start. Now we need to add seasonality to the model. This can be done by adding a sine/cosine terms.\n\n\n Accounting for cyclical trends in models If we knew the origin, then we could just use one Sin term. In this case, it appears the lowest incidence occurs in the summer (August). However, in this example we will estimate the origin with both a sine and cosine term.\nWe can safely assume the period to be 12 months. Let’ save and print the 6 knots locations:\n\n## Save knots locations\nk &lt;- attr(rcs(sicily$time, 6), \"parms\")\nk\n\n[1]  5.00 14.34 24.78 35.22 45.66 55.00\n\n\nNow let’s add the sin/cos function to time and re-fir the model:\n\nkn &lt;- k\nh &lt;- function(x) cbind(rcspline.eval(x, kn), sin = sin(2 * pi * x / 12), cos = cos(2 * pi * x / 12))\n\nf1 &lt;- Glm(aces ~ offset(log(stdpop)) + gTrans(time, h), data = sicily, family = poisson)\nf1$aic\n\n[1] 674.112\n\n\n\nggplot(Predict(f1, fun = g, offset = off)) +\n  geom_point(aes(x = time, y = rate), data = sicily) +\n  geom_vline(aes(xintercept = 37, col = \"red\")) +\n  labs(\n    y = \"Acute Coronary Cases Per 100,000\",\n    x = \"Months since start\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\nAcute Cardiac Events (aces) over time with a sin/cos function of time from a Poisson model adjusted for population size.\n\n\n\n\nThis looks much better already, and the AIC confirms it (674 vs. 722 - lower is better).\n\n\n\nIntervention\nConventional interrupted time series assumes a discontinuity at the intervention point. Since the intervention occurs at month 37 (i.e. January 2005), we start by adding 3 knots at 36, 37 and 38 months. Notice that the sin/cos function has been updated with the additional knots for a toal of 9 knots.\n\nkn &lt;- sort(c(k, c(36, 37, 38)))\nf2 &lt;- Glm(aces ~ offset(log(stdpop)) + gTrans(time, h), data = sicily, family = poisson)\nf2$aic\n\n[1] 661.7904\n\n\n\nggplot(Predict(f2, fun = g, offset = off)) +\n  geom_point(aes(x = time, y = rate), data = sicily) +\n  geom_vline(aes(xintercept = 37, col = \"red\")) +\n  labs(\n    y = \"Acute Coronary Cases Per 100,000\",\n    x = \"Months since start\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\nAcute Cardiac Events (aces) over time with a sin/cos function of time and additional knots around the interventionto allow for a sudden change from a Poisson model adjusted for population size.\n\n\n\n\n\nNine knots is a lot given the sample size of 59. We now simplify the number of knots (6 instead of 9) and add a sudden discontinuity at 37 months:\n\nh &lt;- function(x) {\n  cbind(rcspline.eval(x, k),\n    sin = sin(2 * pi * x / 12), cos = cos(2 * pi * x / 12),\n    jump = x &gt;= 37\n  )\n}\n\nf3 &lt;- Glm(aces ~ offset(log(stdpop)) + gTrans(time, h), data = sicily, family = poisson)\nf3$aic\n\n[1] 659.6044\n\n\n\ntimes &lt;- sort(c(seq(0, 60, length = 200), 36.999, 37, 37.001))\n\nggplot(Predict(f3, time = times, fun = g, offset = off)) +\n  geom_point(aes(x = time, y = rate), data = sicily) +\n  geom_vline(aes(xintercept = 37, col = \"red\")) +\n  labs(\n    y = \"Acute Coronary Cases Per 100,000\",\n    x = \"Months since start\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\nAcute Cardiac Events (aces) over time with a sin/cos function of time and a discontinuity at the intervention from a Poisson model adjusted for population size.\n\n\n\n\n\n\n\nAssessing the Effect\nWe now focus on quantifying the evidence on the effect of the intervention.\n\noptions(prType = \"html\")\nf3\n\n\nGeneral Linear Model\n\nGlm(formula = aces ~ offset(log(stdpop)) + gTrans(time, h), family = poisson, \n    data = sicily)\n\n\n\n\n\n\n\n\n\n\n\nModel Likelihood\nRatio Test\n\n\n\n\nObs 59\nLR χ2 169.64\n\n\nResidual d.f. 51\nd.f. 7\n\n\ng 0.080\nPr(&gt;χ2) &lt;0.0001\n\n\n\n\n\n\n\nβ\nS.E.\nWald Z\nPr(&gt;|Z|)\n\n\n\n\nIntercept\n -6.2118\n 0.0095\n-656.01\n&lt;0.0001\n\n\ntime\n  0.0635\n 0.0113\n5.63\n&lt;0.0001\n\n\ntime'\n -0.1912\n 0.0433\n-4.41\n&lt;0.0001\n\n\ntime''\n  0.2653\n 0.0760\n3.49\n0.0005\n\n\ntime'''\n -0.2409\n 0.0925\n-2.61\n0.0092\n\n\nsin\n  0.0343\n 0.0067\n5.11\n&lt;0.0001\n\n\ncos\n  0.0380\n 0.0065\n5.86\n&lt;0.0001\n\n\njump\n -0.1268\n 0.0313\n-4.06\n&lt;0.0001\n\n\n\n\n\n\nFrom the table of coefficients we see strong evidence in support of the intervention. In particular, the coefficient for the jump is \\(-0.127\\). Because Poisson regression models the log of the expected count, the coefficient reflects the difference in log-counts of acute cardiac event associated with the intervention. Since the difference is negative, it means that the intervention is protective (i.e. it appears to be working). We can make the coefficient more interpretable by exponentiating it and say that the intervention is associated with a \\(\\left [1-exp^{-0.127} \\right ]*100 = 11.9\\%\\) reduction in acute cardiac event.\n\n\n\n\n\n\n\nConclusion\n\n\n\nAccording to the model, the intervention is associated with a \\(12\\%\\) reduction in acute cardiac event, and between \\(6\\%\\) and \\(17\\%\\), 95% of the times.\n\n\n\n\n\nImproving the estimate\nCalculations of robust standard errors to be used in the 95% confidence interval above.\n\n## Source: https://stats.oarc.ucla.edu/r/dae/poisson-regression/\nlibrary(sandwich)\nf3.1 &lt;- glm(aces ~ offset(log(stdpop)) + gTrans(time, h), data = sicily, family = poisson)\n\ncov.m1 &lt;- vcovHC(f3.1, type = \"HC0\")\nstd.err &lt;- sqrt(diag(cov.m1))\nr.est &lt;- cbind(Estimate = coef(f3.1), \"Robust SE\" = std.err, \"Pr(&gt;|z|)\" = 2 * pnorm(abs(coef(f3.1) / std.err), lower.tail = FALSE), LL = coef(f3.1) - 1.96 * std.err, UL = coef(f3.1) + 1.96 * std.err)\nexp(r.est[8, -3])\n\n Estimate Robust SE        LL        UL \n0.8808974 1.0337250 0.8254514 0.9400677 \n\n\n\n\n\nReal World Example\nEarlier, I alluded to the application of this method to the analysis of a technology intervention for in-situ well. Indeed, this post was written when I wanted re-analyze the data using this modified technique. Naturally, I cannot share neither the example nor the findings."
  },
  {
    "objectID": "colophon.html#graphs",
    "href": "colophon.html#graphs",
    "title": "Colophon",
    "section": "Graphs",
    "text": "Graphs\nGraphs follow a similar style.\n\ntheme_set(theme_minimal(base_family = \"Pragmata Pro Mono\") +\n            theme(\n              plot.title = element_text(family = \"Lato\", face = \"bold\"),\n              plot.subtitle = element_text(family = \"Lato\"),\n              axis.text = element_text(family = \"Pragmata Pro Mono\"),\n              axis.title = element_text(family = \"Lato\", hjust = 1),\n              panel.spacing = unit(2, \"lines\"),\n              strip.text.x = element_text(family = \"Lato\"),\n              strip.background =element_rect(fill = \"Grey90\"),\n              panel.grid.minor = element_blank(),\n              panel.grid.major = element_line(size = 0.2),\n              legend.text = element_text(family = \"Lato\"),\n              legend.title = element_text(family = \"Lato\", face = \"bold\"),\n              panel.background = element_rect(fill = '#FFFFF8', color = '#FFFFF8'),\n              plot.background = element_rect(fill = \"#FFFFF8\", color = '#FFFFF8')\n            ))"
  },
  {
    "objectID": "posts/2023-08-28-prediction-vs-inference/index.html#resources",
    "href": "posts/2023-08-28-prediction-vs-inference/index.html#resources",
    "title": "Prediction vs. Inference Cultures: Same Toolbox, Different Focus",
    "section": "Resources",
    "text": "Resources\n\nhttps://arxiv.org/abs/1811.10154\n\nhttps://med.stanford.edu/content/dam/sm/dbds/documents/biostats-workshop/paper-1-.pdf\nhttps://www.fharrell.com/post/split-val/\n\nBerk, Richard A. Statistical Learning from a Regression Perspective. Springer Texts in Statistics. Cham: Springer International Publishing, 2020. https://doi.org/10.1007/978-3-030-40189-4."
  },
  {
    "objectID": "posts/2023-08-28-prediction-vs-inference/index.html#conclusion",
    "href": "posts/2023-08-28-prediction-vs-inference/index.html#conclusion",
    "title": "Prediction vs. Inference Cultures: Same Toolbox, Different Focus",
    "section": "Conclusion",
    "text": "Conclusion\nIt certainly feels like we have moved from one extreme to the other: for more than a century we have focused on the data culture, that is, inference, unbiased estimates, causation and data generating processes. Breiman hoped the field would pay more attention to algorithmic (i.e. pure prediction) approaches. Little would he know (he died in 2005), that the world seems to be overwhelmingly focused on prediction and prediction only today. This is not to say prediction is not important. Far from it. Rather, that the vast majority of practitioners are blind to inference."
  }
]